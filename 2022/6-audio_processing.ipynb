{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/6-audio_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvz7-jSpTLYJ"
      },
      "source": [
        "# Librerías necesarias"
      ],
      "id": "Nvz7-jSpTLYJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka777dPfiC74"
      },
      "source": [
        "!pip install python_speech_features\n",
        "!pip install hmmlearn\n",
        "!pip install tflearn"
      ],
      "id": "ka777dPfiC74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3UH85Z7SVyE"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa # for working with audio in python\n",
        "import librosa.display # for waveplots, spectograms, etc\n",
        "import soundfile as sf # for accessing file information\n",
        "from librosa.feature import mfcc\n",
        "from hmmlearn import hmm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import tflearn\n",
        "import tensorflow as tf"
      ],
      "id": "f3UH85Z7SVyE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv3p4NQITW_h"
      },
      "source": [
        "#Base de datos de dígitos\n",
        "En estas actividades utilizaremos una base de datos de dígitos pronunciados por 25 personas, donde cada persona pronunció 3 veces cada dígito.\n",
        "\n",
        "Los archivos de tiene la siguiente estructura XLLR.wav\n",
        "donde X indica el dígito pronunciado, LL el locutor y R la repeteción. \n",
        "\n",
        "Ejemplo: 3122.wav corresponde a la segunda pronuncación del dígito 3 por el locutor 12.\n"
      ],
      "id": "Wv3p4NQITW_h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCZPr7hniQxF"
      },
      "source": [
        "Descargar base de datos"
      ],
      "id": "fCZPr7hniQxF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQCLc2XAiHG2"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1A-Ab5T1sUu6D29NifSyY16ZYe_6m2Jle' -O digitos.zip\n",
        "!unzip digitos.zip > /dev/null"
      ],
      "id": "CQCLc2XAiHG2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6waLPNFCiNby"
      },
      "source": [
        "Esta base de datos puede representarse en un dataframe de la siguiente manera."
      ],
      "id": "6waLPNFCiNby"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VtmguVLV4j-"
      },
      "source": [
        "database_path = \"./digitos/\"\n",
        "filenames = os.listdir(database_path)\n",
        "df = pd.DataFrame()\n",
        "df['filename'] = filenames\n",
        "df['full_filename'] = database_path + df.filename\n",
        "df['label'] = df.filename.str[0]"
      ],
      "id": "0VtmguVLV4j-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyW2Aez1URkh"
      },
      "source": [
        "### Leer archivo de audio"
      ],
      "id": "WyW2Aez1URkh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKs0vRv1USeS"
      },
      "source": [
        "wave_file = './digitos/0011.wav'\n",
        "info = sf.info(wave_file)\n",
        "x, sample_rate = librosa.load(wave_file, sr=None)\n",
        "print('Duración:', info.duration, 'secs')\n",
        "print('Frecuencia de muestreo:', info.duration, 'Hz')\n",
        "print('Tamaño:', info.frames, 'muestras')"
      ],
      "id": "hKs0vRv1USeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY8Z-pXrUYF8"
      },
      "source": [
        "### Visualizar forma de onda"
      ],
      "id": "NY8Z-pXrUYF8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlyNVE6TUZA9"
      },
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(x, sr=sample_rate)"
      ],
      "id": "BlyNVE6TUZA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu-4w5ZGUcmd"
      },
      "source": [
        "### Visualizar espectrograma"
      ],
      "id": "eu-4w5ZGUcmd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWr9EDHzUd5L"
      },
      "source": [
        "fig = plt.figure(figsize=(18, 5))\n",
        "hop_length = 256  # Ventana FFT\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(x, hop_length=hop_length)), ref=np.max)\n",
        "img = librosa.display.specshow(D, y_axis='linear', x_axis='time', hop_length=hop_length, sr=sample_rate)\n",
        "fig.colorbar(img, ax=fig.axes, format=\"%+2.f dB\")"
      ],
      "id": "jWr9EDHzUd5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-tC8A7tUhbs"
      },
      "source": [
        "# Actividad 1: Comparar espectrogramas\n",
        "El espectrograma contiene información asociada al tipo de palabra pronunciada.\n",
        "\n",
        "Palabras diferentes tendrán espectros diferentes. En cambio, los espectros de una misma palabra pronunciada por personas diferentes presentan similitudes.\n",
        "\n",
        "Si bien estas diferencias y/o similitudes pueden resultar difíciles de identificar mediante la simple inspección de los espectros, estas características pueden ser utilizadas para diferenciar/clasificar palabras."
      ],
      "id": "e-tC8A7tUhbs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H9nfpbHUpGf"
      },
      "source": [
        "# Esta función permite vizualizar los espectrogramas de las señales de audio de entrada\n",
        "def comparar_espectro(filename1, filename2):\n",
        "    x1, sample_rate1 = librosa.load(filename1, sr=None)\n",
        "    x2, sample_rate2 = librosa.load(filename2, sr=None)\n",
        "    sample_rate = sample_rate1  # Para este caso son iguales\n",
        "\n",
        "    hop_length = 256  # Ventana FFT\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n",
        "\n",
        "    D1 = librosa.amplitude_to_db(np.abs(librosa.stft(x1, hop_length=hop_length)), ref=np.max)\n",
        "    img = librosa.display.specshow(D1, y_axis='linear', x_axis='time', sr=sample_rate, hop_length=hop_length, ax=ax[0])\n",
        "    ax[0].label_outer()\n",
        "\n",
        "    D2 = librosa.amplitude_to_db(np.abs(librosa.stft(x2, hop_length=hop_length)), ref=np.max)\n",
        "    librosa.display.specshow(D2, y_axis='linear', sr=sample_rate, hop_length=hop_length, x_axis='time', ax=ax[1])\n",
        "    ax[1].label_outer()\n",
        "    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
      ],
      "id": "1H9nfpbHUpGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bfuytowUtLo"
      },
      "source": [
        "* Visualice y compare los espectros asociados a diferentes pronunciaciones de una misma palabra.\n",
        "* Realice la misma comparación para pronunciaciones de palabras diferentes\n",
        "* ¿Qué diferencias/similitudes puede observar?"
      ],
      "id": "2bfuytowUtLo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuA2lmxCU8WB"
      },
      "source": [
        "#Actividad 2: Entrenar un modelo para cada clase en la base de datos\n",
        "\n",
        "Una representación más compacta del espectro de una señal de voz viene dada por sus parámetros MFCC. Los mismos pueden calcularse utilizando la función *mfcc(signal, sample_rate, n_mfcc, win_length)* donde signal es la señal de audio, sample_rate su frecuencia de muestreo, n_mfcc la cantidad de parámetros a calcular y win_length la ventana utilizada dividir en cuadros la señal\n",
        "Ejemplo:\n",
        "\n",
        "*signal, sample_rate = librosa.load('./digitos/0011.wav', sr=None)*\n",
        "\n",
        "mfcc_features = mfcc(signal, sample_rate, n_mfcc=12, win_length=4*160).T"
      ],
      "id": "CuA2lmxCU8WB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiJAFvrAVK2e"
      },
      "source": [
        "### 2.1: Crear una función que, a partir de una archivo de audio, calcule sus parámetros MFCC asociados"
      ],
      "id": "IiJAFvrAVK2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0pyg2AyVMUX"
      },
      "source": [],
      "id": "V0pyg2AyVMUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-_oS2Q2VPLE"
      },
      "source": [
        "### 2.2: Calcular los MFCC para todas las palabras de la base de datos.\n",
        "Hint: df['features'] = df.apply(fun, axis=1)\n"
      ],
      "id": "v-_oS2Q2VPLE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCTDGdEQVR33"
      },
      "source": [],
      "id": "YCTDGdEQVR33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhK8ZZLSVZ45"
      },
      "source": [
        "### 2.2: Separar los datos en conjuntos de train y test, manteniendo la misma proporción de clases en ambos conjuntos. Verifique la cantidad de ejemplos en cada subconjunto.\n",
        "Hint: Realizar un slip estratificado"
      ],
      "id": "rhK8ZZLSVZ45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQhx4WJrVbwb"
      },
      "source": [],
      "id": "fQhx4WJrVbwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zExO2ZJVdt8"
      },
      "source": [
        "### 2.3: Entrenar modelos\n",
        "Para entrenar los modelos se utiliza la función *hmm.GMMHMM()* de la librería *hmmlearn*. Esta función toma los siguientes parámetros:\n",
        "\n",
        "* *n_components*: cantidad de estados del modelo\n",
        "* *n_mix*: cantidad de Gaussianas en cada estado\n",
        "* *n_iter*: iteraciones máximas para entrenar modelo\n",
        "\n",
        "Ejemplo:"
      ],
      "id": "-zExO2ZJVdt8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ctGDjukVk3y"
      },
      "source": [
        "states_num = 5\n",
        "GMM_mix_num = 3\n",
        "model = hmm.GMMHMM(n_components=states_num, n_mix=GMM_mix_num, n_iter=200)\n",
        "trainData = train_dataset[train_dataset.label == '0']['features']\n",
        "trData = np.vstack(trainData)\n",
        "model.fit(trData)\n",
        "# Dada una secuencia de features asociadas a una determinada señal de voz, se puede calcular el score asociado al un modelo de la siguiente manera\n",
        "feature_matrix = test_dataset.iloc[0]['features']\n",
        "score = model.score(feature_matrix)"
      ],
      "id": "6ctGDjukVk3y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHzF7eYsVoN1"
      },
      "source": [
        "* Cree una función que devuelva un diccionario que contenga los modelos entranados para cada clase utilizando datos de entrenamiento. Esta función debe tomar como argumentos de entrada el dataframe con datos de entrenamiento, la cantidad de estados y mezclas Gaussiandas que tendrán los modelos"
      ],
      "id": "ZHzF7eYsVoN1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W93oF4fOVpFz"
      },
      "source": [],
      "id": "W93oF4fOVpFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Bhutul3a7P"
      },
      "source": [
        "* Entrene los modelos utilizando 5 estados y 3 mezclas Gaussianas"
      ],
      "id": "D5Bhutul3a7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1UEk8lb3lb2"
      },
      "source": [],
      "id": "o1UEk8lb3lb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdBlBdTlVqbN"
      },
      "source": [
        "* Calcule el accuracy de los modelos entrenados sobre los datos de test. Para cada ejemplo de: test, calcule el score asociado a cada modelo entrenado y seleccione la clase que tiene mayor score\n"
      ],
      "id": "wdBlBdTlVqbN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqlTxjuVVu-d"
      },
      "source": [],
      "id": "UqlTxjuVVu-d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGekkdIE4Jgw"
      },
      "source": [
        "* Entrene modelos con diferentes cantidad de estados y mezclas Gaussianas. ¿Cómo resulta el accuarcy al variar estos parámetros?"
      ],
      "id": "dGekkdIE4Jgw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nwJMyCQof2T"
      },
      "id": "0nwJMyCQof2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clasificación usando LSTM"
      ],
      "metadata": {
        "id": "DRS-K7JBHU-c"
      },
      "id": "DRS-K7JBHU-c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este caso necesitamos:\n",
        "* Entradas de longitud fija\n",
        "* One-hot encoding"
      ],
      "metadata": {
        "id": "Qa0jLiXDLncA"
      },
      "id": "Qa0jLiXDLncA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función completa con 0 o recorta las caracterísitcas del audio calculadas anteriormente dependiendo del valor de utterance_length\n",
        "def mfcc_long_fija(mfcc_features, utterance_length):\n",
        "    mfcc_features = mfcc_features.T\n",
        "    if mfcc_features.shape[1] > utterance_length:\n",
        "        mfcc_features = mfcc_features[:, 0:utterance_length]\n",
        "    else:\n",
        "        mfcc_features = np.pad(mfcc_features, ((0, 0), (0, utterance_length - mfcc_features.shape[1])),\n",
        "                               mode='constant', constant_values=0)\n",
        "    \n",
        "    return mfcc_features"
      ],
      "metadata": {
        "id": "4cLD36xDHt4P"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4cLD36xDHt4P"
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode para cada dígito\n",
        "df['one_hot'] = df.apply(lambda x: np.eye(10)[int(x.label)], axis=1)\n",
        "utterance_length = 35\n",
        "df['features_long_fija'] = df.apply(lambda x: mfcc_long_fija(x.features, utterance_length), axis=1)"
      ],
      "metadata": {
        "id": "rpz9Oi8hL9Vf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rpz9Oi8hL9Vf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos datos de train, validación y test\n",
        "train_dataset, test_dataset = train_test_split(df, test_size=0.3, stratify=df.label, random_state=333)\n",
        "train_dataset2, val_dataset = train_test_split(train_dataset, test_size=0.3, stratify=train_dataset.label)\n",
        "X_tr = train_dataset2.features_long_fija.to_list() \n",
        "y_tr = train_dataset2.one_hot.to_list() \n",
        "X_val = val_dataset.features_long_fija.to_list() \n",
        "y_val = val_dataset.one_hot.to_list()\n"
      ],
      "metadata": {
        "id": "o1hlidtFMSlG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "o1hlidtFMSlG"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "# Parámetros\n",
        "lr = 0.001\n",
        "audio_features = 12  \n",
        "ndigits = 10\n",
        "# Creamos el modelo\n",
        "sp_network = tflearn.input_data([None, audio_features, utterance_length], name='input')\n",
        "sp_network = tflearn.lstm(sp_network, 128*4, dropout=0.5)\n",
        "sp_network = tflearn.fully_connected(sp_network, ndigits, activation='softmax')\n",
        "sp_network = tflearn.regression(sp_network, optimizer='adam', learning_rate=lr, loss='categorical_crossentropy')\n",
        "sp_model = tflearn.DNN(sp_network)"
      ],
      "metadata": {
        "id": "xgWlBh8wMIYZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xgWlBh8wMIYZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo\n",
        "bsize = 32\n",
        "sp_model.fit(X_tr, y_tr, n_epoch=40, validation_set=(X_val, y_val), show_metric=True, batch_size=bsize)"
      ],
      "metadata": {
        "id": "6yet1MjBhjp1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6yet1MjBhjp1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculamos accurary sobre test"
      ],
      "metadata": {
        "id": "0jfjxHiQdJ-l"
      },
      "id": "0jfjxHiQdJ-l"
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['pred_label'] = None\n",
        "for index, row in test_dataset.iterrows():\n",
        "    feature = row['features_long_fija']\n",
        "    feature = feature.reshape((1, feature.shape[0], feature.shape[1]))\n",
        "    prediction_digit = sp_model.predict(feature)\n",
        "    predict = str(np.argmax(prediction_digit))\n",
        "    test_dataset.at[index, 'pred_label'] = predict\n",
        "accuracy = accuracy_score(test_dataset.label, test_dataset.pred_label)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "metadata": {
        "id": "rwTnQoEWN_EI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rwTnQoEWN_EI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Evalue como resulta el accuracy al utilizar diferentes valores para *utterance_length*\n",
        "* ¿Qué debería tener en cuenta para seleccionar un valor a priori para esta variable?"
      ],
      "metadata": {
        "id": "GV33LC6hmmTU"
      },
      "id": "GV33LC6hmmTU"
    }
  ]
}