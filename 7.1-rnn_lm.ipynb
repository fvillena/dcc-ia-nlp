{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.1-rnn_lm",
      "provenance": [],
      "authorship_tag": "ABX9TyNTyZiAhBvKPGKhDTfIJMEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/7.1-rnn_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XdIgYpMWr_B"
      },
      "source": [
        "# Redes recurrentes\n",
        "\n",
        "Deep learning es un tipo de aprendizaje de máquinas en donde las predicciones se realizan a través de una serie de operaciones matriciales concatenadas. Uno de los elementos más importantes en el Deep Learning es la casi nula ingeniería de características. En el ejemplo que veremos ahora, se utiliza un conjunto de imágenes segmentadas para realizar el entrenamiento de un modelo de segmentación automática de núcleos celulares, este entrenamiento simplemente se realiza con los pixeles crudos de las imágenes, sin ingeniería de características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyDFW9FC-q-4"
      },
      "source": [
        "import tensorflow as tf # Biblioteca de redes neuronales\n",
        "import os # Módulo para interactuar con el sistema operativo"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0e17pcmXHLR"
      },
      "source": [
        "Importamos el conjunto de datos. El conjunto de datos es una colección de diagnósticos médicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCKnzmir--E8",
        "outputId": "cf82add3-9b98-437d-dd5a-54afb84ac69f"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('corpus.txt', 'https://raw.githubusercontent.com/fvillena/workshopEmbeddingsAndClassifiers/master/corpus.txt')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/fvillena/workshopEmbeddingsAndClassifiers/master/corpus.txt\n",
            "7397376/7394290 [==============================] - 0s 0us/step\n",
            "7405568/7394290 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbmwCB8_Gvm"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8').lower() # Leemos el archivo descargado"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zsTntNC_LNr",
        "outputId": "dd3ead34-afff-4208-dd66-453e186f5646"
      },
      "source": [
        "len(text) # Tamaño del corpus"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7373422"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM797bic_QCc",
        "outputId": "7366e76c-2dee-4216-a6a7-2a5a0045c81f"
      },
      "source": [
        "print(text[:250]) # Estos son los primeros 250 caracteres de nuestro corpus."
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de sus estructuras de sosten\n",
            "trastornos de disco lumbar y otros, con radiculopatia\n",
            "celulitis y absceso de boca\n",
            "pitiriasis alba\n",
            "fisura anal\n",
            "periodontitis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h04PrazXsSt"
      },
      "source": [
        "El vocabulario de nuestro conjunto de datos son todos los caracteres distintos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOBzo7-_S53"
      },
      "source": [
        "vocab = sorted(set(text)) "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGsPfeOq_Vr2",
        "outputId": "b92c3fa7-ff69-4571-c74a-598cd518aaf5"
      },
      "source": [
        "len(vocab) # Tamaño del vocabulario"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s7jlSGtYaNg"
      },
      "source": [
        "Transformamos el corpus a un conjunto de identificadores de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COZZw0DL_bVa"
      },
      "source": [
        "# Esta función transforma una cadena de caracteres en una lista de identificadores de caracteres\n",
        "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ4l-Snk_7EE"
      },
      "source": [
        "# Esta función realiza el procedimiento inverso a la función anterior\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_DcT3ru_o33"
      },
      "source": [
        "# Esta función concatena los caracteres en un string.\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "605ebhCN_rPj",
        "outputId": "02185ef8-2804-46cc-ca62-deb1b45b50c8"
      },
      "source": [
        "# Transformamos nuestro corpus.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7373422,), dtype=int64, numpy=array([23, 25, 32, ...,  2, 11,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYFZAGEZ_tK_"
      },
      "source": [
        "# Transformamos los identificadores en un objeto que Tensorflow puede leer.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9_jP5bSZVnC"
      },
      "source": [
        "Así se ven los primeros caracteres de nuestro conjunto de datos preprocesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeVkmxAd_wlI",
        "outputId": "308db9cd-c9dd-4684-fe8d-9b134a8831a7"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "e\n",
            "l\n",
            "u\n",
            "l\n",
            "i\n",
            "t\n",
            "i\n",
            "s\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIV9zrMwZmNp"
      },
      "source": [
        "Construiremos nuestro conjunto de datos para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr5pDitt_yJw"
      },
      "source": [
        "seq_length = 100 # Esta es la cantidad de caracteres que tendrá cada uno de nuestros ejemplos.\n",
        "# examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O3lkBeaACuj"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # Generamos las secuencias de entrenamiento"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0JXklm1aRx6"
      },
      "source": [
        "Así se ve las primeras secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2JFEksuaMFJ",
        "outputId": "f1aaffc6-1ca2-46a5-e290-dcf478b47433"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy().decode())\n",
        "  print(\"---\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de \n",
            "---\n",
            "sus estructuras de sosten\n",
            "trastornos de disco lumbar y otros, con radiculopatia\n",
            "celulitis y absceso d\n",
            "---\n",
            "e boca\n",
            "pitiriasis alba\n",
            "fisura anal\n",
            "periodontitis apical aguda originada en la pulpa\n",
            "osteomielitis, no\n",
            "---\n",
            " especificada\n",
            "examen de pesquisa especial para tumor del cuello uterino\n",
            "insuficiencia renal crónica\n",
            "i\n",
            "---\n",
            "nsuficiencia renal crónica\n",
            "periodontitis cronica\n",
            "herida de la muñeca y de la mano\n",
            "otros trastornos de\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk1Ogi9pAHVP"
      },
      "source": [
        "# Con esta función transformamos cada secuencia en textos de entrada y de salida para el entrenamiento.\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7lwLHgAN_0"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0t9gbHHbUFh"
      },
      "source": [
        "Así se ve el texto de entrada y salida del primer ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdAd0nygAQFQ",
        "outputId": "82ab0f78-119a-4d55-9e0d-1400ef1176f3"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Entrada :\\n\", text_from_ids(input_example).numpy().decode())\n",
        "    print(\"---\")\n",
        "    print(\"Salida :\\n\", text_from_ids(target_example).numpy().decode())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrada :\n",
            " celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de\n",
            "---\n",
            "Salida :\n",
            " elulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtByxqyMb2IM"
      },
      "source": [
        "Así preparamos el conjunto de datos para comenzar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsNZDpvyAWT_",
        "outputId": "99f14d96-4c92-44d4-dad8-4edd5e233827"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znFZEaYbAYOF"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9gJbMaocIKr"
      },
      "source": [
        "Creamos la clase que definirá nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiIpsMQqAadU"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # Esta es la capa de embedding\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, # Capa GRU\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # Esta es la capa de salida que predecirá el siguiente caracter.\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training) # Acá transformamos nuestra entrada en su representación de embedding.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training) # Acá pasamos por GRU nuestra secuencia representada \n",
        "    x = self.dense(x, training=training) # Pasamos la salida de GRU por nuestra capa de clasificación\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8RkIx61czJW"
      },
      "source": [
        "Instanciamos nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FCtQrmAc4z"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg8_l7ncdsNn"
      },
      "source": [
        "Estas son las dimensiones de un batch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J43xDYxydHbY",
        "outputId": "b6dca09f-5057-4b9a-cf0e-2cf6e39d076d"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 60) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzTc7wjc74J"
      },
      "source": [
        "Así se ve la estructura de nuestra red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l2PK2upAgyz",
        "outputId": "15f7069f-28e9-4a84-9979-032eedd03fb2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  15360     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  61500     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,015,164\n",
            "Trainable params: 4,015,164\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz3P2xQmd8to"
      },
      "source": [
        "Así se ve una predicción del modelo sin entrenar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrn5kqoCAkNj",
        "outputId": "c1c40924-acd2-440f-bc59-2e05afe979bf"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Entrada:\\n\", text_from_ids(input_example_batch[0]).numpy().decode())\n",
        "print()\n",
        "print(\"Predicción:\\n\", text_from_ids(sampled_indices).numpy().decode())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrada:\n",
            " ardiaca, no especificada\n",
            "diabetes mellitus no insulinodependiente\n",
            "artralgias multiples\n",
            "venas varicos\n",
            "\n",
            "Predicción:\n",
            " hsmë/z:a0yq0ì-ìwéoq.öfk3r[UNK]:íz[UNK]ápb0(g:k,01-†w.4c8ó,n+xk)ak/+l8,b-ómsyn5v7g)do.tfyf(dc6ææxñs0mk7qi--2w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGyCfgubeNfi"
      },
      "source": [
        "Configuramos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxTGMeNnAwIk"
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YqjzDF8AzgT"
      },
      "source": [
        "EPOCHS = 5"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry_qZ0rZefmO"
      },
      "source": [
        "Ajustamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsh3s0IIA4n0",
        "outputId": "3e3e5d76-cc4c-4627-b305-fcaa6986a52e"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1140/1140 [==============================] - 151s 130ms/step - loss: 0.6836\n",
            "Epoch 2/5\n",
            "1140/1140 [==============================] - 150s 131ms/step - loss: 0.3004\n",
            "Epoch 3/5\n",
            "1140/1140 [==============================] - 171s 148ms/step - loss: 0.2729\n",
            "Epoch 4/5\n",
            "1140/1140 [==============================] - 170s 147ms/step - loss: 0.2613\n",
            "Epoch 5/5\n",
            "1140/1140 [==============================] - 156s 135ms/step - loss: 0.2553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mmyw206A6Mn"
      },
      "source": [
        "# Con esta clase podemos generar texto de tamaño cualquiera data una entrada.\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask, validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4BIBDA3A-G7"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afXKS0ocgNu-"
      },
      "source": [
        "Generamos texto sintético. Se puede observar que el modelo aprendió a generar una lista de diagnósticos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf0zjErmBAsm",
        "outputId": "ce2a94ac-e422-4358-c5b1-9a83e144ae08"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['neoplasia']) # Esta es la secuencia de entrada.\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neoplasia congenita del enocertil\n",
            "observacion por sospecha de enfermedad o afeccion no especificada\n",
            "observaci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdTW_9NhqWi"
      },
      "source": [
        "Probamos con varias palabras de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJASrENqBG2V",
        "outputId": "43b2b3dd-7bca-47b1-d38e-e445a8d8cf51"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['neoplasia', 'periodontitis', 'virus', 'infección', 'fractura'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "for r in result:\n",
        "  print(r.numpy().decode())\n",
        "  print(\"---\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neoplasiata no especificada\n",
            "consulta no especificada\n",
            "consulta no especificada\n",
            "periodontitis cronica\n",
            "insuficie\n",
            "---\n",
            "periodontitis cronica severa\n",
            "secuelas de traumatismo debido a traumatismo de la movula\n",
            "observacion por sospecha d\n",
            "---\n",
            "virus\n",
            "periodontitis apical aguda originada en la pulpa\n",
            "otros trastornos del ojo y sus anexos\n",
            "observacion \n",
            "---\n",
            "infecciónintomica\n",
            "síndrome de laceracion y hemorragia, no especificada\n",
            "retinopatia diabetica\n",
            "hipoacusia, no e\n",
            "---\n",
            "fracturartal\n",
            "consulta no especificada\n",
            "atencion medica, no especificada\n",
            "consulta no especificada\n",
            "consulta no \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXpxZlGBUIz"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    }
  ]
}