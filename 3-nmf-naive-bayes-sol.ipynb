{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-nmf-naive-bayes-sol.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRMBlbn0cxXRV0id6rMJ60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/3-nmf-naive-bayes-sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPLHoW2f3FXb"
      },
      "source": [
        "## NMF y Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGhIFxg3JPf"
      },
      "source": [
        "Preparamos el ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIIlVDiMU5ir",
        "outputId": "9ef7b3c0-af43-4ed7-8894-be6bc60bbfb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://files.fast.ai/data/examples/imdb_sample.tgz\n",
        "!tar -xvf imdb_sample.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 22:14:25--  https://files.fast.ai/data/examples/imdb_sample.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 172.67.69.159, 104.26.3.19, 104.26.2.19, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|172.67.69.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571827 (558K) [application/x-gtar-compressed]\n",
            "Saving to: ‘imdb_sample.tgz’\n",
            "\n",
            "imdb_sample.tgz     100%[===================>] 558.42K   945KB/s    in 0.6s    \n",
            "\n",
            "2020-11-05 22:14:26 (945 KB/s) - ‘imdb_sample.tgz’ saved [571827/571827]\n",
            "\n",
            "imdb_sample/\n",
            "imdb_sample/texts.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migNakfPVFL5"
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "import sklearn.feature_extraction\n",
        "import sklearn.decomposition\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zptCLItUt0ey",
        "outputId": "c10844f9-a748-4ff3-f293-08e79f17980a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "remove = ('headers', 'footers', 'quotes')\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups = sklearn.datasets.fetch_20newsgroups(remove=remove,categories=categories)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hzuGx_at-hq"
      },
      "source": [
        "# Actividad 1: NMF\n",
        "\n",
        "Desde la matriz de tf-idf calculada anteriormente sobre newsgroups realice una descomposición mediante NMF.\n",
        "\n",
        "1.   Verifique hasta que vector los temas identificados en NMF hacen “sentido”  para los 4 temas seleccionados.\n",
        "3.   Compare su resultado con SVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_kAc4KxeZdf"
      },
      "source": [
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=\"english\",min_df=0.005)\n",
        "x = tfidf_vectorizer.fit_transform(newsgroups.data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke0xrtIKt9_8",
        "outputId": "ce4087cf-0e72-4f35-9d84-37759c5edf79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = tfidf_vectorizer.get_feature_names()\n",
        "def show_topics(a):\n",
        "  num_top_words=8\n",
        "  top_words_values = lambda t: [f\"{vocab[i]}({str(round(v,3))})\" for i,v in zip(np.argsort(t)[:-num_top_words-1:-1],np.sort(t)[:-num_top_words-1:-1])]\n",
        "  topic_words_values = ([top_words_values(t) for t in a])\n",
        "  return [' '.join(t) for t in topic_words_values]\n",
        "#NMF\n",
        "d=5 # num topics\n",
        "clf = sklearn.decomposition.NMF(n_components=d, random_state=1)\n",
        "W1 = clf.fit_transform(x)\n",
        "H1 = clf.components_\n",
        "show_topics(H1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don(0.709) people(0.686) think(0.672) just(0.648) like(0.426) say(0.335) know(0.321) time(0.294)',\n",
              " 'thanks(0.611) graphics(0.496) files(0.417) image(0.381) file(0.347) program(0.315) know(0.298) format(0.279)',\n",
              " 'space(1.097) nasa(0.462) launch(0.28) shuttle(0.278) orbit(0.228) lunar(0.222) moon(0.22) earth(0.22)',\n",
              " 'god(1.44) jesus(0.628) bible(0.413) believe(0.28) christian(0.265) does(0.233) atheism(0.224) christ(0.217)',\n",
              " 'com(0.501) ico(0.46) tek(0.456) bobbe(0.449) bronx(0.442) beauchaine(0.442) manhattan(0.442) vice(0.44)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-8nh26x3M5u"
      },
      "source": [
        "Para este paso práctico utilizaremos el conjunto de datos de imbd en donde tenemos una serie de documentos etiquetados con la polaridad (negativo o positivo) asociada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46O65JHiVLID"
      },
      "source": [
        "imdb = pd.read_csv(\"imdb_sample/texts.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5vLm4BVTIC",
        "outputId": "72aa61b0-df28-4b1c-b65a-30ef4ed26583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "imdb"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>negative</td>\n",
              "      <td>There are many different versions of this one ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>positive</td>\n",
              "      <td>Once upon a time Hollywood produced live-actio...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>negative</td>\n",
              "      <td>Wenders was great with Million $ Hotel.I don't...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>negative</td>\n",
              "      <td>Although a film with Bruce Willis is always wo...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>positive</td>\n",
              "      <td>A compelling, honest, daring, and unforgettabl...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                               text  is_valid\n",
              "0    negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1    positive  This is a extremely well-made film. The acting...     False\n",
              "2    negative  Every once in a long while a movie will come a...     False\n",
              "3    positive  Name just says it all. I watched this movie wi...     False\n",
              "4    negative  This movie succeeds at being one of the most u...     False\n",
              "..        ...                                                ...       ...\n",
              "995  negative  There are many different versions of this one ...      True\n",
              "996  positive  Once upon a time Hollywood produced live-actio...      True\n",
              "997  negative  Wenders was great with Million $ Hotel.I don't...      True\n",
              "998  negative  Although a film with Bruce Willis is always wo...      True\n",
              "999  positive  A compelling, honest, daring, and unforgettabl...      True\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFMhQl654otz"
      },
      "source": [
        "Para construir nuestra matriz de terminos y documentos utilizademos la clase CountVectorizer de sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDlj_BPVTxm"
      },
      "source": [
        "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=\"english\")\n",
        "term_doc_matrix_train = count_vectorizer.fit_transform(imdb[imdb.is_valid == False].text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkADeQ342sX"
      },
      "source": [
        "Nuestra matriz de términos y documentos tiene la forma (documentos,términos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2m6WMydah5-",
        "outputId": "5aecb6e3-2c0d-46ad-bc86-fb94dca511e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "term_doc_matrix_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x16163 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 74815 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6cIEgnx5D2Y"
      },
      "source": [
        "Calculamos la cantidad de apariciones de cada una de las palabras en cada una de las clases, junto con la cantidad de apariciones de cada clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HNlpcwnYq0q"
      },
      "source": [
        "c_w_negative = np.asarray(term_doc_matrix_train.todense()[imdb[imdb.is_valid == False].label == \"negative\",:].sum(0)).reshape(-1) # apariciones de cada una de las palabras en la clase negativa\n",
        "c_w_positive = np.asarray(term_doc_matrix_train.todense()[imdb[imdb.is_valid == False].label == \"positive\",:].sum(0)).reshape(-1)\n",
        "c_negative = (imdb[imdb.is_valid == False].label == \"negative\").sum() # apariciones de la clase negativa\n",
        "c_positive = (imdb[imdb.is_valid == False].label == \"positive\").sum()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKtEe64I5tzq"
      },
      "source": [
        "## Actividad 2: Palabras más comunes por clase\n",
        "\n",
        "Dados los arreglos que contienen la cantidad de apariciones de cada palabra en cada clase, imprima las 3 palabras más comunes para cada clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNFKgIThg4e7",
        "outputId": "fab9bc1e-7112-40df-9c96-82bd9f96e84e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = count_vectorizer.get_feature_names()\n",
        "print(np.array(vocab)[np.argsort(c_w_negative)][-3:])\n",
        "print(np.array(vocab)[np.argsort(c_w_positive)][-3:])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['film' 'movie' 'br']\n",
            "['movie' 'film' 'br']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sInCqV6aNL"
      },
      "source": [
        "## Actividad 3: Ratio positivo - negativo\n",
        "\n",
        "Calcule el ratio positivo negativo en esta base de datos, y muestre cuales son las 3 palabras más probables en cada clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu10qRxZxiyH"
      },
      "source": [
        "p_w_negative = (c_w_negative + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa\n",
        "p_w_positive = (c_w_positive + 1) / c_positive "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CfdxWdbfED9",
        "outputId": "219912a6-c77a-4595-de1f-5ee0060f13b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Programa\n",
        "r = np.log(p_w_positive / p_w_negative)\n",
        "print(np.array(vocab)[np.argsort(r)][:20])\n",
        "print(np.array(vocab)[np.argsort(r)][-20:])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['porn' 'vargas' 'crap' 'naschy' 'dog' 'crater' 'worst' 'disappointment'\n",
            " 'fuqua' 'soderbergh' 'scorsese' 'johnson' 'jackman' 'poorly' 'junior'\n",
            " 'rangers' 'lake' 'bey' 'bet' 'dinosaur']\n",
            "['steals' 'arnold' 'damon' '3d' 'hanzo' 'alfred' 'cheech' 'howard' 'sport'\n",
            " 'paxton' 'gilliam' 'jabba' 'davies' 'han' 'felix' 'jimmy' 'noir'\n",
            " 'astaire' 'fanfan' 'biko']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H34eASrZ61Vz"
      },
      "source": [
        "## Actividad 4: Clasificador\n",
        "\n",
        "Ahora calcule el log ratio de cada documento. Si asumimos que > 0 es opinión positiva, ¿cuál es el accuracy de la clasificación?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nLjuXOc68Tn",
        "outputId": "17080404-494e-466b-86a4-a8dd5e31add7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Programa\n",
        "# term_doc_matrix_train.shape\n",
        "term_doc_matrix_test = count_vectorizer.transform(imdb[imdb.is_valid == True].text)\n",
        "test_pred = term_doc_matrix_test.dot(r) > 0\n",
        "test_label = np.where(imdb[imdb.is_valid == True].label == \"positive\", True, False)\n",
        "confusion_matrix = pd.crosstab(test_label,test_pred)\n",
        "print(np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmCv7Qu5vhPb"
      },
      "source": [
        "## Actividad 5: Bigramas\n",
        "\n",
        "Genere un nuevo clasificador, esta vez agregando bigramas al vocabulario de la matriz de términos y documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52BUv28Ovq00",
        "outputId": "400eb41f-c4d2-4802-c631-66e13f046f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Programa\n",
        "count_vectorizer_bigrams = sklearn.feature_extraction.text.CountVectorizer(stop_words=\"english\",ngram_range=(1,2))\n",
        "term_doc_matrix_train_bigrams = count_vectorizer_bigrams.fit_transform(imdb[imdb.is_valid == False].text)\n",
        "c_w_negative_bigrams = np.asarray(term_doc_matrix_train_bigrams.todense()[imdb[imdb.is_valid == False].label == \"negative\",:].sum(0)).reshape(-1) # apariciones de cada una de las palabras en la clase negativa\n",
        "c_w_positive_bigrams = np.asarray(term_doc_matrix_train_bigrams.todense()[imdb[imdb.is_valid == False].label == \"positive\",:].sum(0)).reshape(-1)\n",
        "p_w_negative_bigrams = (c_w_negative_bigrams + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa\n",
        "p_w_positive_bigrams = (c_w_positive_bigrams + 1) / c_positive \n",
        "r_bigrams = np.log(p_w_positive_bigrams / p_w_negative_bigrams)\n",
        "term_doc_matrix_test_bigrams = count_vectorizer_bigrams.transform(imdb[imdb.is_valid == True].text)\n",
        "test_pred_bigrams = term_doc_matrix_test_bigrams.dot(r_bigrams) > 0\n",
        "confusion_matrix_bigrams = pd.crosstab(test_label,test_pred_bigrams)\n",
        "print(np.array(confusion_matrix_bigrams).diagonal().sum() / np.array(confusion_matrix_bigrams).sum())"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tq41m4gokX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}