{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxCnx/t0BaNv/iTls7AwWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/5-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPcd3UfJ5ec"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcmrlhNLkSC"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import logging"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pc9wPn4vQgL"
      },
      "source": [
        "Descargamos el corpus Brown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxoUU72HiAEb",
        "outputId": "005aeb64-c5ac-4a56-c653-aa56cd90cc01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2y8Lb66OMHs"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoP29UJM60q"
      },
      "source": [
        "corpus = nltk.corpus.brown.sents()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqhjOSpvBUF"
      },
      "source": [
        "Entrenamos un modelo model sobre el corpus Brown, el cual contiene alrededor de 1.000.000 de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFDEXJlNDxV",
        "outputId": "cdec0035-c00b-4eb2-848f-30d9f2a71ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = gensim.models.word2vec.Word2Vec(sentences = corpus)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-07 03:31:26,399 : INFO : collecting all words and their counts\n",
            "2020-11-07 03:31:26,408 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-11-07 03:31:26,866 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
            "2020-11-07 03:31:27,296 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
            "2020-11-07 03:31:27,747 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
            "2020-11-07 03:31:28,170 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
            "2020-11-07 03:31:28,518 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
            "2020-11-07 03:31:28,784 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
            "2020-11-07 03:31:28,785 : INFO : Loading a fresh vocabulary\n",
            "2020-11-07 03:31:28,909 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
            "2020-11-07 03:31:28,910 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
            "2020-11-07 03:31:28,961 : INFO : deleting the raw counts dictionary of 56057 items\n",
            "2020-11-07 03:31:28,965 : INFO : sample=0.001 downsamples 42 most-common words\n",
            "2020-11-07 03:31:28,966 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
            "2020-11-07 03:31:29,011 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
            "2020-11-07 03:31:29,012 : INFO : resetting layer weights\n",
            "2020-11-07 03:31:31,890 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-11-07 03:31:32,912 : INFO : EPOCH 1 - PROGRESS: at 25.39% examples, 204167 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:33,919 : INFO : EPOCH 1 - PROGRESS: at 50.16% examples, 211293 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:34,936 : INFO : EPOCH 1 - PROGRESS: at 77.47% examples, 212027 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:35,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-07 03:31:35,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-07 03:31:35,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-07 03:31:35,627 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 3.7s, 209612 effective words/s\n",
            "2020-11-07 03:31:36,659 : INFO : EPOCH 2 - PROGRESS: at 25.39% examples, 201562 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:37,679 : INFO : EPOCH 2 - PROGRESS: at 50.16% examples, 208552 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:38,691 : INFO : EPOCH 2 - PROGRESS: at 77.47% examples, 210579 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:39,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-07 03:31:39,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-07 03:31:39,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-07 03:31:39,396 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 3.8s, 207646 effective words/s\n",
            "2020-11-07 03:31:40,409 : INFO : EPOCH 3 - PROGRESS: at 25.39% examples, 205725 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:41,435 : INFO : EPOCH 3 - PROGRESS: at 50.16% examples, 210200 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:42,457 : INFO : EPOCH 3 - PROGRESS: at 77.47% examples, 210913 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:43,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-07 03:31:43,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-07 03:31:43,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-07 03:31:43,134 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 3.7s, 209408 effective words/s\n",
            "2020-11-07 03:31:44,151 : INFO : EPOCH 4 - PROGRESS: at 25.39% examples, 205384 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:45,163 : INFO : EPOCH 4 - PROGRESS: at 50.16% examples, 211391 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:46,168 : INFO : EPOCH 4 - PROGRESS: at 77.47% examples, 212835 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:46,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-07 03:31:46,878 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-07 03:31:46,882 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-07 03:31:46,884 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 3.7s, 208891 effective words/s\n",
            "2020-11-07 03:31:47,911 : INFO : EPOCH 5 - PROGRESS: at 25.39% examples, 203294 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:48,920 : INFO : EPOCH 5 - PROGRESS: at 50.16% examples, 210714 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:49,937 : INFO : EPOCH 5 - PROGRESS: at 77.47% examples, 211568 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-07 03:31:50,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-07 03:31:50,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-07 03:31:50,626 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-07 03:31:50,628 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 3.7s, 209304 effective words/s\n",
            "2020-11-07 03:31:50,629 : INFO : training on a 5805960 raw words (3908888 effective words) took 18.7s, 208608 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ntZpYYBvGFc"
      },
      "source": [
        "Importamos un modelo de word2vec entrenado sobre google news, este modelo fue entrenado sobre un corpus de 3.000.000.000 de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnP7Rh5jVpk",
        "outputId": "7be1df9b-db9b-4085-f2a1-0ac436805676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "news = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-07 03:31:50,912 : INFO : loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "2020-11-07 03:32:42,467 : INFO : loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REu7SPwCrKK2"
      },
      "source": [
        "## Actividad 1: Exploración de los embeddings\n",
        "\n",
        "Después de calcular los embeddings, podemos asociar un vector de números reales, de dimensiones conocidas a una palabra de nuestro vocabulario.\n",
        "\n",
        "\n",
        "1.   ¿Cuántas dimensiones tiene cada vector asociado a las palabras en cada uno de los modelos?\n",
        "2.   ¿Cuántos vectores hay en cada uno de los modelos?\n",
        "3.   ¿Existen diferencian en el tamaño del vocabulario del modelo `model` y el modelo news?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RtInP-wrHEn"
      },
      "source": [
        "# HINT\n",
        "# news.wv.vectors # esta es una matriz"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxFNhxZSoZ0"
      },
      "source": [
        "## Actividad 2: Similaridad y analogía\n",
        "\n",
        "Existen un par de cualidades semánticas de las palabras, las cuales pueden ser fácilmente demostradas a través de operaciones vectoriales sobre el espacio generado por el proceso de cálculo de embeddings.\n",
        "\n",
        "La similaridad es la métrica de cercanía que tienen 2 palabras, esta característica es fácil de representar a través de la similaridad coseno entre 2 vectores.\n",
        "\n",
        "La analogía es la relación semántica que tienen 2 palabras, por ejemplo, la palabra \"rey\" y \"reina\" están relacionadas por el concepto de \"género\". Estas analogías se pueden operacionalizar en el espacio vectorial de los word embeddings como la resta de los vectores asociados a las palabras.\n",
        "\n",
        "\n",
        "\n",
        "1.   Verifique cuáles son las palabras más cercanas a palabras que usted seleccione e interprete la correctitud de las palabras retornadas por los modelos `model` y `news`\n",
        "2.   Verifique cuál de los modelos resuelve mejor la prueba de analogía: \"man\" es a \"woman\" como \"king\" es a \"queen\". Invente otra analogía y pruebe si el modelo la puede resolver.\n",
        "3.   Según sus pruebas, ¿el tamaño del corpus de entrenamiento afecta el rendimiento del modelo?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fQ0w9XlNOHE",
        "outputId": "da7965ac-cb91-4c97-b67d-69fff471d723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# así obtenemos las palabras más cercanas a las palabra \"woman\"\n",
        "model.wv.most_similar(\"woman\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-07 03:32:42,484 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.9567420482635498),\n",
              " ('boy', 0.9006858468055725),\n",
              " ('child', 0.8777297735214233),\n",
              " ('man', 0.8770748972892761),\n",
              " ('young', 0.8689302206039429),\n",
              " ('distaste', 0.8673049807548523),\n",
              " ('remark', 0.8612951040267944),\n",
              " ('old', 0.8565317392349243),\n",
              " ('fellow', 0.8559356927871704),\n",
              " ('artist', 0.8551065921783447)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ov7uKOqrGR"
      },
      "source": [
        "#HINT:\n",
        "# news.wv.vectors[news.vocab[\"woman\"].index] # así obtenemos el vector asociado a la palabra woman\n",
        "# news.wv.similar_by_vector(vector) # así obtenemos las palabras más cercanas a un vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj4Hv0fkPKXg"
      },
      "source": [
        "## Actividad 3: Reducción de dimensionalidad y visualización\n",
        "\n",
        "Nuestros vectores tienen más de 2 dimensiones, por lo que no es muy fácil interptretar su ubicación espacial. Utiliza un método de reducción de dimensionalidad que conozcas para reducir hacia 2 dimensiones nuestros vectores y visualiza el resultado.\n",
        "\n",
        "\n",
        "*   ¿Existen agrupaciones aparentes de palabras dentro del espacio? y si es así, ¿las palabras agrupadas, son similares?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JhwXqJJeXaD"
      },
      "source": [
        "# prográmame\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}