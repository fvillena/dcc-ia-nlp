{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iW239eziMR7"
      },
      "source": [
        "# Tarea 1\n",
        "\n",
        "### Cuerpo Docente\n",
        "\n",
        "- Profesores: [Andrés Abeliuk](https://aabeliuk.github.io/), [Felipe Villena](https://fabianvillena.cl/).\n",
        "- Profesora Auxiliar: María José Zambrano\n",
        "\n",
        "### Instrucciones generales\n",
        "\n",
        "- Grupos de máximo 4 personas.\n",
        "- Ausentes deberán realizar la actividad solos.\n",
        "- Esta prohibido compartir las respuestas con otros grupos.\n",
        "- Indicios de copia serán penalizados con la nota mínima.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente, si utiliza material extra debe citarlo.\n",
        "\n",
        "\n",
        "### Integrantes\n",
        "\n",
        "> POR FAVOR AGREGAR TODOS LOS NOMBRES DE LOS INTEGRANTES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvRYIoMgiiNn"
      },
      "source": [
        "# Preguntas Teoricas\n",
        "\n",
        "1. ¿Por qué es importante realizar un preprocesamiento adecuado antes de aplicar modelos de aprendizaje automático en NLP?\n",
        "  - *Respuesta*\n",
        "\n",
        "2. Imagina que trabajas con un corpus grande y sin limpiar (por ejemplo, tweets). ¿Qué pasos adicionales incluirías en el preprocesamiento para mejorar la calidad del análisis?\n",
        "  - *Respuesta*\n",
        "\n",
        "3. Explica brevemente la diferencia entre lematización y stemming.\n",
        "  - *Respuesta*\n",
        "\n",
        "4. Si una palabra aparece con mucha frecuencia en un corpus, ¿crees que siempre es relevante? Justifica tu respuesta.\n",
        "  - *Respuesta*\n",
        "\n",
        "5. Explica la principal diferencia entre las representaciones Bag of Words y TF-IDF.\n",
        "  - *Respuesta*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iljXpaTrLSsG"
      },
      "source": [
        "# Parte Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HLEsoDKj5zM"
      },
      "source": [
        "## Pregunta 1\n",
        "En muchas ocasiones, las bibliotecas necesitan inventariar la información de todos los libros disponibles en su stock. Dado que la información puede estar sin procesar o contener errores, se te solicita, en tu calidad de estudiante del módulo de Procesamiento del Lenguaje Natural (NLP), que revises la información contenida en el archivo libros.txt.\n",
        "\n",
        "Para revisar la información debe realizar los siguientes pasos usando sólo funciones nativas de Python:\n",
        "\n",
        "- Leer el Archivo: Abrir el archivo libros.txt y leer su contenido.\n",
        "- Transformación de Strings: Convertir los títulos de los libros y los nombres de los autores a mayúsculas.\n",
        "- Búsqueda de Strings: Encontrar y mostrar todos los libros que contienen la palabra 'la' en el título.\n",
        "- Split de Strings: Para cada autor, extraer y mostrar solo su primer nombre.\n",
        "- Reemplazo de Strings: Reemplazar cualquier instancia de 'y' por '&' en los títulos de los libros y mostrar los títulos modificados.\n",
        "- Longitud de Strings: Calcular y mostrar la cantidad de palabras en el título de cada libro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQi_jPmAL8iY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ruh7qpMvsI"
      },
      "source": [
        "## Pregunta 2\n",
        "\n",
        " a) Diseñe una función **`get_vocab()`** que extraiga los tokens de este corpus solamente tokenizando.\n",
        " dataset = [\"I like human languages\", \"I like programming languages\", \"Spanish is my favorite language\"]\n",
        "``\n",
        "``\n",
        "\n",
        "***Resultado esperado***:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>['favorite',\n",
        " 'Spanish',\n",
        " 'language',\n",
        " 'I',\n",
        " 'like',\n",
        " 'programming',\n",
        " 'languages',\n",
        " 'my',\n",
        " 'human',\n",
        " 'is'] </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "``\n",
        "``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr6Ig3yvwWEH"
      },
      "outputs": [],
      "source": [
        "dataset = [\"I like human languages\", \"I like programming languages\", \"Spanish is my favorite language\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUczCQ1kMvHS"
      },
      "outputs": [],
      "source": [
        "def get_vocab(dataset):\n",
        "  ### Aquí inicia tu código ###\n",
        "\n",
        "  ...\n",
        "\n",
        "  ### Aquí termina tu código ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPSLm_LDOu6o"
      },
      "source": [
        "b) 4 - WorldCloud - Seleccione un corpus de texto de la librería **`nltk`**, y cree un word cloud de las principales palabras. Para esto corra la siguiente línea y elija un corpus.\n",
        "\n",
        "De acuerdo, a los resultados obtenidos, considera que el WorldCloud refleja el contenido principal del que habla un documento, si es así ¿Porqué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsjDZmPsXjMs",
        "outputId": "d33eff62-617b-44e3-f4ed-398da1930235"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDuI6a-3OvDk",
        "outputId": "49aba493-8b1b-4e4d-f79e-35478e2057ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "print(nltk.corpus.gutenberg.fileids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBIKUH4tiMSH"
      },
      "source": [
        "## Pregunta 3\n",
        "### Contexto\n",
        "\n",
        "El discurso de odio es cualquier expresión que promueva o incite a la discriminación, la hostilidad o la violencia hacia una persona o grupo de personas en una relación asimétrica de poder, tal como la raza, la etnia, el género, la orientación sexual, la religión, la nacionalidad, una discapacidad u otra característica similar.\n",
        "\n",
        "En cambio, la incivilidad se refiere a cualquier comportamiento o actitud que rompe las normas de respeto, cortesía y consideración en la interacción entre personas. Esta puede manifestarse de diversas formas, tal como insultos, ataques personales, sarcasmo, desprecio, entre otras.\n",
        "\n",
        "En esta tarea tendrán a su disposición un dataset de textos con las etiquetas `odio`, `incivilidad` o `normal`. La mayor parte de los datos se encuentra en español de Chile. Con estos datos, deberán entrenar un modelo que sea capaz de predecir la etiqueta de un texto dado.\n",
        "\n",
        "El corpus para esta tarea se compone de 3 datasets:  \n",
        "- [Multilingual Resources for Offensive Language Detection de Arango et al. (2022)](https://aclanthology.org/2022.woah-1.pdf#page=136)\n",
        "- [Dataton UTFSM No To Hate (2022)](http://dataton.inf.utfsm.cl/)\n",
        "- Datos generados usando la [API de GPT3 (modelo DaVinci 03)](https://platform.openai.com/docs/models/gpt-3).\n",
        "\n",
        "Agradecimientos a los autores por compartir los datos y a David Miranda, Fabián Diaz, Santiago Maass y Jorge Ortiz por revisar y reetiquetar los datos en el contexto del curso \"Taller de Desarrollo de Proyectos de IA\" (CC6409), Departamento de Ciencias de la Computación, Universidad de Chile.\n",
        "\n",
        "Los datos solo pueden ser usados con fines de investigación y docencia. Está prohibida la difusión externa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZjrxFD2iMSI"
      },
      "source": [
        "#### Para el siguiente informe deben:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcwwfIjriMSJ"
      },
      "source": [
        "Para la siguente tarea deberá revolver una tarea de multi-clasificación, realizando los siguentes pasos:\n",
        "\n",
        "- Realizar un análisis estadístico del corpus.\n",
        "- Crear dos tipos de text representation.\n",
        "- Desarrollar al menos cuatros tipos de clasificadores que resuelvan la tarea.\n",
        "- Análizar sus resultados.\n",
        "\n",
        "Sin embargo, primero cargaremos el datataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkPVO6ESiMSK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvB2UE62iMSM"
      },
      "source": [
        "### Cargar dataset\n",
        "\n",
        "En esta sección, cargaremos el dataset desde el repositorio del módulo. Para ello ejecute las siguientes líneas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rSTfQ2AiMSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFJiPdc9iMSO"
      },
      "outputs": [],
      "source": [
        "# Dataset de entrenamiento.\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtg92w0iiMSP"
      },
      "source": [
        "### Analizar los datos\n",
        "\n",
        "En esta sección analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi7mS3yViMSQ",
        "outputId": "4138dbf3-e97c-4376-a4df-b14333240c73"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9466</th>\n",
              "      <td>13085</td>\n",
              "      <td>La recalcada putísima de mierda y la concha de...</td>\n",
              "      <td>odio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5578</th>\n",
              "      <td>7613</td>\n",
              "      <td>@user @user No es que los gaymers chilenos sea...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2204</th>\n",
              "      <td>11375</td>\n",
              "      <td>@user Dios bendiga a #Colombia y autoridades @...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9202</th>\n",
              "      <td>15259</td>\n",
              "      <td>Cuando me enteré de que Shakira estaba saliend...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3271</th>\n",
              "      <td>10230</td>\n",
              "      <td>@user claro si los q la sacaron fueron los per...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                              texto   clase\n",
              "9466  13085  La recalcada putísima de mierda y la concha de...    odio\n",
              "5578   7613  @user @user No es que los gaymers chilenos sea...  normal\n",
              "2204  11375  @user Dios bendiga a #Colombia y autoridades @...  normal\n",
              "9202  15259  Cuando me enteré de que Shakira estaba saliend...  normal\n",
              "3271  10230  @user claro si los q la sacaron fueron los per...  normal"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhdWKPEZiMSR",
        "outputId": "2091318c-79eb-4545-d05d-c17fe75a98c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "incivilidad    5424\n",
              "normal         4280\n",
              "odio           2510\n",
              "Name: clase, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"clase\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLHWoi4TiMSS"
      },
      "source": [
        "### Instalar librerias\n",
        "\n",
        "Debe instalar las siguientes librerías:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-AfecWmiMSS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi_mJozgiMST"
      },
      "source": [
        "Si lo desea, puede instalar más librería que requiera, pero debe citar su fuente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfqkdt0kiMST"
      },
      "source": [
        "### Importar librerías\n",
        "\n",
        "En esta sección, importamos la liberías necesarias para el correcto desarrollo de esta tarea. Puede utilizar otras librerías que no se en encuentran aquí, pero debe citar su fuente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4JcaTc0iMST"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.text import Text\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# importe aquí sus clasificadores\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxbSG538iMSU"
      },
      "source": [
        "### Análisis estadístico\n",
        "\n",
        "Para entender como esta compuesto el corpus de texto, y los principales conceptos que aborda, haremos un análisis estadístico de sus principales componentes, para esto se le pide:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_NJtqNuiMSU"
      },
      "source": [
        "#### 1. Tokenizador\n",
        "\n",
        "Como vimos en clases pasadas, el como separar los principales token de una oración no siempre es una tarea fácil. Por lo que para separar los principales token del texto asociado a cada sentimiento se pide que defina un tokenizador, que entregue una lista de los principales tokens. Su tokenizador debe contener al menos tres expresiones regulares, que usted estime conveniente para hacer la separación de los token dentro de la oración, para ello complete le siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk4uTb07iMSU"
      },
      "outputs": [],
      "source": [
        "def tokenizador(oracion):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gv7CB0giMSV"
      },
      "source": [
        "En el siguiente ejemplo, se muestra un comportamiento esperado de su tokenizador, usando como ejemplo el `word_tokenize` de la librería `nltk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ibM0v0JiMSV",
        "outputId": "35ca9cd5-e8ea-4617-d3d2-37f7a00a0b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'el',\n",
              " 'único',\n",
              " 'que',\n",
              " 'se',\n",
              " 'ha',\n",
              " 'dado',\n",
              " 'cuenta',\n",
              " 'que',\n",
              " 'con',\n",
              " 'la',\n",
              " 'llegada',\n",
              " 'de',\n",
              " 'extranjeros',\n",
              " 'a',\n",
              " 'Chile',\n",
              " ',',\n",
              " 'los',\n",
              " 'asesinatos',\n",
              " 'aumentaron',\n",
              " 'en',\n",
              " 'numero',\n",
              " 'y',\n",
              " 'en',\n",
              " 'nivel',\n",
              " 'de',\n",
              " 'violencia',\n",
              " '.',\n",
              " 'Ya',\n",
              " 'cacharon',\n",
              " 'que',\n",
              " 'aquí',\n",
              " 'esta',\n",
              " 'el',\n",
              " '🧀']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ejemplo\n",
        "\n",
        "oracion = df['texto'].sample(1).values[0]\n",
        "word_tokenize(oracion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyFsRmFhiMSV"
      },
      "source": [
        "Ahora pruebe su tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GGjvlZ8iMSW"
      },
      "outputs": [],
      "source": [
        "oracion = df['texto'].sample(1).values[0]\n",
        "tokenizador(oracion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x31twODfiMSW"
      },
      "source": [
        "Ahora responda, su tokenizador funciona igual que el provisto por la librería, y si es así explique porque, en caso contrario, indique las causa de porque no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR7kyQF7iMSW"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h14PvAq-iMSW"
      },
      "source": [
        "#### 2) Crear una lista de tokens.\n",
        "\n",
        "Dado que nos interesa entender cuales son los token que más impacto tienen dentro del corpus de texto provisto, debemos extraerlos del conjunto de entrenamiento. Para esto, guarde en la lista TODOS los token del conjunto de entrenamiento en la lista `tokens`, para separar los tokens de la oraciones utilice su tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWZX3lwhiMSX"
      },
      "outputs": [],
      "source": [
        "tokens = []\n",
        "\n",
        "# Código (recuerde utilizar su tokenizador)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9SKRF7IiMSY"
      },
      "source": [
        "Para asegurarse que guardo correctamente las palabras, extraiga los primeros 10 tokens de la lista, y revise el largo de la lista. Ejecutando la siguiente línea de código, ojo que el largo de la lista `tokens` debe ser cercano o mayor 300000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td5hq4c4iMSY"
      },
      "outputs": [],
      "source": [
        "print(len(tokens))\n",
        "print(tokens[:11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlH6YZoSiMSY"
      },
      "source": [
        "#### 3) Análisis estadísticos los tokens.\n",
        "\n",
        "Para realizar nuestro análisis utilizaremos las primeras técnicas en el curso:\n",
        "\n",
        "- a) Cree un gráfico de dispersión lexica (ver tutorial 2) sobre al menos 10 token escogidos al azar de la lista `tokens`, podría serle útil usar el módulo `random` de `Python`. Luego, repita el mismo gráfico con al menos 10 `tokens` que usted considere interesante dentro del contexto del dataset. ¿Puede observar algún tipo de relación entre los tokens escogidos por usted, y los escogidos azar? Explique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT3tJVMSiMSZ"
      },
      "outputs": [],
      "source": [
        "# Grafico de tokens escogidos al azar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn_RbltQiMSZ"
      },
      "outputs": [],
      "source": [
        "# Grafico de tokens escogidos por usted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1B4_xV_iMSZ"
      },
      "source": [
        "- b) Para estudiar los tokens más frequentes dentro del corpus, cree dos gráfico de frecuencias (ver tutorial 2), uno con todos los tokens de la lista, y otro eliminandos las stopwords. ¿Qué puede decir sobre estos gráficos?, ¿Existe alguna diferencia al mantener las stopwords vs a quitarlas de los tokens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKoeX7KliMSZ"
      },
      "outputs": [],
      "source": [
        "# Grafico 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDG5VxLsiMSa"
      },
      "outputs": [],
      "source": [
        "# Grafico 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2pAVxl_iMSa"
      },
      "source": [
        "- c) Dado que siempre es importante, identificar los principales tokens mencionados en un corpus de texto, cree dos wordclouds (ver tutorial 2) sobre los tokens que obtuvo en la parte anterior, uno con stopwords y otro sin ellas. ¿Qué puede decir sobre estos gráficos?, ¿Existe alguna diferencia al mantener las stopwords vs a quitarlas de los tokens?, considerando la parte a) existe alguna diferencia versus las palabras que usted pensó que era más interesantes?, que puede decir al respecto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAeRR1YZiMSa"
      },
      "outputs": [],
      "source": [
        "# Grafico 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J8AfuINiMSa"
      },
      "outputs": [],
      "source": [
        "# Grafico 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_XtXb8OiMSb"
      },
      "source": [
        "## Crear representaciones de texto\n",
        "\n",
        "Como hemos mencionado en las clases anteriores, los modelos de Machine Learning no son capaces de entender el texto directamente para resolver cualquier tarea. Es vital realizar un paso intermedio, que es buscar un mecanismo que permita traducir el texto a representaciones que puedan ser entendidas por los modelos de ML, y que conserven las propiedades semánticas y sintacticas del lenguaje. Es por esto, que en está sección estudiaremos los métodos de text representation estudiados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_yZoSVGiMSj"
      },
      "source": [
        "- a) El primer método que se les solicita, es crear una representación de Bag of Words o TF-IDF (la que prefieran) utilizando la librería `scikit-learn`. Para esto, puede serle útil revisar el tutorial tres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpeWWc6kiMSj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_n64dVDiMSk"
      },
      "source": [
        "- b) El segundo método que se les solicita, es crear una representación con word embeddings. Para esto, puede serle útil revisar el tutorial cuatro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYhF6Hm3iMSk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQa25HSiMSk"
      },
      "source": [
        "## Definir clasificadores\n",
        "\n",
        "En esta parte, se procedera a entrenar, los clasificadores por cada representación de texto, creada en la parte anterior, para esto usted debe:\n",
        "\n",
        "Se sugiere revisar el tutorial 4, para recordar como utilizar los clasificadores de `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK8o4yBBiMSk"
      },
      "source": [
        "1. Definir los datos y las etiquetas de entrenamiento y testeo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-nGf6z9iMSk"
      },
      "outputs": [],
      "source": [
        "#separar df en entrenamiento y testing\n",
        "X_train = ...\n",
        "y_train = ...\n",
        "\n",
        "X_test = ...\n",
        "y_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXAlliHiMSk"
      },
      "source": [
        "Una pregunta, que podría hacerse: ¿Es necesario separar el dataset en training y testing, usando alguna función de `scikit-learn` en este caso? Fundamente:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUE_5S4hiMSl"
      },
      "source": [
        "2. Definir al menos dos clasificadores por representación:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxKJODgIiMSl"
      },
      "source": [
        "#### Bag of Words o TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwR3ZnN0iMSl"
      },
      "outputs": [],
      "source": [
        "clf1 = ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDFbFKTDiMSl"
      },
      "outputs": [],
      "source": [
        "clf2 = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gquhv9F7iMSl"
      },
      "source": [
        "#### Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACUnu6LUiMSl"
      },
      "outputs": [],
      "source": [
        "clf3 = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDdqCVCKiMSl"
      },
      "outputs": [],
      "source": [
        "clf4 = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tyvfoYYiMSl"
      },
      "source": [
        "Para esta sección pueden utilizar cualquier clasificadores que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvUDccA9iMSm"
      },
      "source": [
        "3. Entrenar cada uno de los clasificadores desarrollados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-pTSK9hiMSm"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento classificador 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip6clmmziMSm"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento classificador 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO1K16xdiMSm"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento classificador 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEfGBGlxiMSn"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento classificador 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy5zMy26iMSn"
      },
      "source": [
        "### Evaluar los clasificadores.\n",
        "\n",
        "En esta sección, se espera que entregue la matriz de confusión y el reporte de clasificación de los clasificadores en la sección pasada. Por lo que seria útil estudiar las funciones `confusion_matrix` y `classification_report` de `scikit-lear`. Podrá encontrar un ejemplo en el tutorial 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIjer8z_iMSn"
      },
      "outputs": [],
      "source": [
        "# Informe de evaluación clasificador 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV26PdMviMSn"
      },
      "outputs": [],
      "source": [
        "# Informe de evaluación clasificador 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8VkoD_AiMSn"
      },
      "outputs": [],
      "source": [
        "# Informe de evaluación clasificador 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bix6uD2OiMSo"
      },
      "outputs": [],
      "source": [
        "# Informe de evaluación clasificador 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjNWyooviMSo"
      },
      "source": [
        "Finalmente, ¿Qué pude decir del rendimiento de todos los clasificadores?, ¿Cree que alguna representación pudo resolver mejor la tarea? Jusfique, se espera que de un análsis para cada uno de los 4 clasificadores, identificado sus aciertos y fallas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
