{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoqoDrDUNqzP/uLN7KaI2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/4-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPcd3UfJ5ec"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sonMA0Csl_Z2"
      },
      "source": [
        "# Actividad 1: Word2vec en tensorflow\n",
        "\n",
        "Observe el preprocesamiento que se realiza al corpus, la estructura construida con Tensorflow y el procedimiento para entrenar el modelo.\n",
        "\n",
        "\n",
        "1.   Construya los word embeddings desde los pesos y sesgos del modelo entrenado\n",
        "2.   Visualice los embeddings bidimensionales en un gráfico de dispersión e interprete los resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zak8IEVRl8JD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrljKgFmJFj"
      },
      "source": [
        "corpus = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VnMZGnBmS4V"
      },
      "source": [
        "def preprocess(text):\n",
        "  tokens = text.split(\" \")\n",
        "  return [token for token in tokens if len(token) > 2]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6vpojsnI9L"
      },
      "source": [
        "def get_vocabulary(corpus):\n",
        "  vocabulary = []\n",
        "  for text in corpus:\n",
        "    vocabulary += text\n",
        "  return sorted(list(set(vocabulary)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InbdYdnmslf"
      },
      "source": [
        "sentences = list(map(preprocess, corpus))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaiaxx3Cm_eh"
      },
      "source": [
        "vocabulary = get_vocabulary(sentences)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCP-PFhln7wE"
      },
      "source": [
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        # Acá se toma una ventana de -WINDOWS_SIZE, WINDOWS_SIZE para generar el skip gram. Dado que las \n",
        "        # frases son cortas, se utiliza min y max para tener cuidado con los límites de la frase. \n",
        "        # Además, el +1 en el límite superior es para considerar el índice de la propia palabra en cuestión \n",
        "        # (probar qué ocurre cuando se elimina dicho +1)\n",
        "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1]: \n",
        "            if neighbor != word:\n",
        "                data.append((word, neighbor))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVGhl12TpPEc"
      },
      "source": [
        "features = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "labels = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "for i,(feature,label) in enumerate(data):\n",
        "  features[i,vocabulary.index(feature)] = 1\n",
        "  labels[i,vocabulary.index(label)] = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1a0Y-szpRZ"
      },
      "source": [
        "class Word2Vec:\n",
        "  def __init__(self, vocab_size=0, embedding_dim=2, epochs=10000):\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.epochs=epochs\n",
        "    self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "  def train(self, x_train=None, y_train=None):\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
        " \n",
        "    self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
        " \n",
        "    for _ in range(self.epochs):\n",
        "      with tf.GradientTape() as t:\n",
        "        hidden_layer = tf.add(tf.matmul(x_train,self.W1),self.b1) \n",
        "        output_layer = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, self.W2), self.b2))\n",
        "        cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
        " \n",
        "      grads = t.gradient(cross_entropy_loss, [self.W1, self.b1, self.W2, self.b2])\n",
        "      self.optimizer.apply_gradients(zip(grads,[self.W1, self.b1, self.W2, self.b2]))\n",
        "      if(_ % 1000 == 0):\n",
        "        print(cross_entropy_loss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FswiyGsE0ETj",
        "outputId": "8c835b0d-42fd-487f-c3a4-2341737590c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2v = Word2Vec(vocab_size=len(vocabulary), epochs=10000)\n",
        "w2v.train(features, labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(4.559487, shape=(), dtype=float32)\n",
            "tf.Tensor(2.1401541, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9987773, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9156393, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8868088, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8673179, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8495167, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8335234, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8195103, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8072463, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xazPhyRg6Xqi"
      },
      "source": [
        "# programa\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqZEv1-4RL7"
      },
      "source": [
        "## Embeddings con gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcmrlhNLkSC"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import logging"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pc9wPn4vQgL"
      },
      "source": [
        "Descargamos el corpus Brown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxoUU72HiAEb",
        "outputId": "1b43a8d1-d779-47c3-f8d4-c67f5b3e5305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2y8Lb66OMHs"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoP29UJM60q"
      },
      "source": [
        "corpus = nltk.corpus.brown.sents()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqhjOSpvBUF"
      },
      "source": [
        "Entrenamos un modelo model sobre el corpus Brown, el cual contiene alrededor de 1.000.000 de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFDEXJlNDxV",
        "outputId": "50685324-3afb-461e-a942-c3102b11d6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = gensim.models.word2vec.Word2Vec(sentences = corpus)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 22:04:26,955 : INFO : collecting all words and their counts\n",
            "2022-01-17 22:04:26,958 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-01-17 22:04:27,654 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
            "2022-01-17 22:04:28,233 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
            "2022-01-17 22:04:28,899 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
            "2022-01-17 22:04:29,496 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
            "2022-01-17 22:04:29,989 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
            "2022-01-17 22:04:30,348 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
            "2022-01-17 22:04:30,350 : INFO : Loading a fresh vocabulary\n",
            "2022-01-17 22:04:30,411 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
            "2022-01-17 22:04:30,413 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
            "2022-01-17 22:04:30,478 : INFO : deleting the raw counts dictionary of 56057 items\n",
            "2022-01-17 22:04:30,483 : INFO : sample=0.001 downsamples 42 most-common words\n",
            "2022-01-17 22:04:30,487 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
            "2022-01-17 22:04:30,552 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
            "2022-01-17 22:04:30,554 : INFO : resetting layer weights\n",
            "2022-01-17 22:04:34,229 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2022-01-17 22:04:35,248 : INFO : EPOCH 1 - PROGRESS: at 19.05% examples, 158477 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:36,255 : INFO : EPOCH 1 - PROGRESS: at 39.31% examples, 162241 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:37,265 : INFO : EPOCH 1 - PROGRESS: at 58.31% examples, 165794 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:38,270 : INFO : EPOCH 1 - PROGRESS: at 79.88% examples, 163160 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:39,067 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-01-17 22:04:39,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-01-17 22:04:39,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-01-17 22:04:39,092 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 4.9s, 161050 effective words/s\n",
            "2022-01-17 22:04:40,123 : INFO : EPOCH 2 - PROGRESS: at 19.05% examples, 156795 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:41,163 : INFO : EPOCH 2 - PROGRESS: at 39.99% examples, 161879 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:42,210 : INFO : EPOCH 2 - PROGRESS: at 59.73% examples, 165969 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:43,219 : INFO : EPOCH 2 - PROGRESS: at 83.48% examples, 164806 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:43,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-01-17 22:04:43,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-01-17 22:04:43,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-01-17 22:04:43,892 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 4.8s, 163252 effective words/s\n",
            "2022-01-17 22:04:44,911 : INFO : EPOCH 3 - PROGRESS: at 15.37% examples, 124845 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:45,922 : INFO : EPOCH 3 - PROGRESS: at 35.48% examples, 145395 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:46,956 : INFO : EPOCH 3 - PROGRESS: at 55.32% examples, 155377 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:47,978 : INFO : EPOCH 3 - PROGRESS: at 76.21% examples, 156254 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:48,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-01-17 22:04:48,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-01-17 22:04:48,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-01-17 22:04:48,924 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 5.0s, 155528 effective words/s\n",
            "2022-01-17 22:04:49,962 : INFO : EPOCH 4 - PROGRESS: at 19.05% examples, 155765 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:50,995 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 158828 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:52,036 : INFO : EPOCH 4 - PROGRESS: at 58.31% examples, 161869 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:53,068 : INFO : EPOCH 4 - PROGRESS: at 81.06% examples, 160786 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:53,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-01-17 22:04:53,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-01-17 22:04:53,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-01-17 22:04:53,850 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 4.9s, 158997 effective words/s\n",
            "2022-01-17 22:04:54,889 : INFO : EPOCH 5 - PROGRESS: at 19.05% examples, 155656 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:55,919 : INFO : EPOCH 5 - PROGRESS: at 39.99% examples, 162176 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:56,938 : INFO : EPOCH 5 - PROGRESS: at 59.07% examples, 165423 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:57,971 : INFO : EPOCH 5 - PROGRESS: at 82.18% examples, 163409 words/s, in_qsize 0, out_qsize 0\n",
            "2022-01-17 22:04:58,708 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-01-17 22:04:58,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-01-17 22:04:58,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-01-17 22:04:58,725 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 4.9s, 160741 effective words/s\n",
            "2022-01-17 22:04:58,726 : INFO : training on a 5805960 raw words (3908888 effective words) took 24.5s, 159577 effective words/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ntZpYYBvGFc"
      },
      "source": [
        "Importamos un modelo de word2vec entrenado sobre google news, este modelo fue entrenado sobre un corpus de 3.000.000.000 de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnP7Rh5jVpk",
        "outputId": "da918aea-406d-4f23-c1d9-f64c51882b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "news_path = gensim.downloader.load('word2vec-google-news-300', return_path=True)\n",
        "news = gensim.models.KeyedVectors.load_word2vec_format(news_path, binary=True, limit=20000)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 22:04:58,739 : INFO : Creating /root/gensim-data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 22:11:20,091 : INFO : word2vec-google-news-300 downloaded\n",
            "2022-01-17 22:11:20,093 : INFO : loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "2022-01-17 22:11:23,476 : INFO : loaded (20000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REu7SPwCrKK2"
      },
      "source": [
        "## Actividad 2: Exploración de los embeddings\n",
        "\n",
        "Después de calcular los embeddings, podemos asociar un vector de números reales, de dimensiones conocidas a una palabra de nuestro vocabulario.\n",
        "\n",
        "\n",
        "1.   ¿Cuántas dimensiones tiene cada vector asociado a las palabras en cada uno de los modelos?\n",
        "2.   ¿Cuántos vectores hay en cada uno de los modelos?\n",
        "3.   ¿Existen diferencian en el tamaño del vocabulario del modelo `model` y el modelo news?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RtInP-wrHEn"
      },
      "source": [
        "# HINT\n",
        "# news.wv.vectors # esta es una matriz"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxFNhxZSoZ0"
      },
      "source": [
        "## Actividad 3: Similaridad y analogía\n",
        "\n",
        "Existen un par de cualidades semánticas de las palabras, las cuales pueden ser fácilmente demostradas a través de operaciones vectoriales sobre el espacio generado por el proceso de cálculo de embeddings.\n",
        "\n",
        "La similaridad es la métrica de cercanía que tienen 2 palabras, esta característica es fácil de representar a través de la similaridad coseno entre 2 vectores.\n",
        "\n",
        "La analogía es la relación semántica que tienen 2 palabras, por ejemplo, la palabra \"rey\" y \"reina\" están relacionadas por el concepto de \"género\". Estas analogías se pueden operacionalizar en el espacio vectorial de los word embeddings como la resta de los vectores asociados a las palabras.\n",
        "\n",
        "\n",
        "\n",
        "1.   Verifique cuáles son las palabras más cercanas a palabras que usted seleccione e interprete la correctitud de las palabras retornadas por los modelos `model` y `news`\n",
        "2.   Verifique cuál de los modelos resuelve mejor la prueba de analogía: \"man\" es a \"woman\" como \"king\" es a \"queen\". Invente otra analogía y pruebe si el modelo la puede resolver.\n",
        "3.   Según sus pruebas, ¿el tamaño del corpus de entrenamiento afecta el rendimiento del modelo?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fQ0w9XlNOHE",
        "outputId": "837f87bb-cf69-489e-b9d2-362ea11cdaa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# así obtenemos las palabras más cercanas a las palabra \"woman\"\n",
        "model.wv.most_similar(\"woman\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 22:11:23,499 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.9565746784210205),\n",
              " ('boy', 0.8980492353439331),\n",
              " ('child', 0.874050498008728),\n",
              " ('young', 0.8726806640625),\n",
              " ('man', 0.8721031546592712),\n",
              " ('artist', 0.8611582517623901),\n",
              " ('fellow', 0.8606480360031128),\n",
              " ('remark', 0.8602893352508545),\n",
              " ('youngster', 0.8582425117492676),\n",
              " ('distaste', 0.8568153977394104)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ov7uKOqrGR"
      },
      "source": [
        "#HINT:\n",
        "# news.wv.vectors[news.vocab[\"woman\"].index] # así obtenemos el vector asociado a la palabra woman\n",
        "# news.wv.similar_by_vector(vector) # así obtenemos las palabras más cercanas a un vector"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj4Hv0fkPKXg"
      },
      "source": [
        "## Actividad 4: Reducción de dimensionalidad y visualización\n",
        "\n",
        "Nuestros vectores tienen más de 2 dimensiones, por lo que no es muy fácil interptretar su ubicación espacial. Utiliza un método de reducción de dimensionalidad que conozcas para reducir hacia 2 dimensiones nuestros vectores y visualiza el resultado.\n",
        "\n",
        "\n",
        "*   ¿Existen agrupaciones aparentes de palabras dentro del espacio? y si es así, ¿las palabras agrupadas, son similares?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JhwXqJJeXaD"
      },
      "source": [
        "# prográmame\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}