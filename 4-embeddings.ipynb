{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPRviBCOoIUFFzO6O8h0YM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/4-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPcd3UfJ5ec"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sonMA0Csl_Z2"
      },
      "source": [
        "# Actividad 1: Word2vec en tensorflow\n",
        "\n",
        "Observe el preprocesamiento que se realiza al corpus, la estructura construida con Tensorflow y el procedimiento para entrenar el modelo.\n",
        "\n",
        "\n",
        "1.   Construya los word embeddings desde los pesos y sesgos del modelo entrenado\n",
        "2.   Visualice los embeddings bidimensionales en un gráfico de dispersión e interprete los resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zak8IEVRl8JD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrljKgFmJFj"
      },
      "source": [
        "corpus = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VnMZGnBmS4V"
      },
      "source": [
        "def preprocess(text):\n",
        "  tokens = text.split(\" \")\n",
        "  return [token for token in tokens if len(token) > 2]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6vpojsnI9L"
      },
      "source": [
        "def get_vocabulary(corpus):\n",
        "  vocabulary = []\n",
        "  for text in corpus:\n",
        "    vocabulary += text\n",
        "  return sorted(list(set(vocabulary)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InbdYdnmslf"
      },
      "source": [
        "sentences = list(map(preprocess, corpus))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaiaxx3Cm_eh"
      },
      "source": [
        "vocabulary = get_vocabulary(sentences)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCP-PFhln7wE"
      },
      "source": [
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        # Acá se toma una ventana de -WINDOWS_SIZE, WINDOWS_SIZE para generar el skip gram. Dado que las \n",
        "        # frases son cortas, se utiliza min y max para tener cuidado con los límites de la frase. \n",
        "        # Además, el +1 en el límite superior es para considerar el índice de la propia palabra en cuestión \n",
        "        # (probar qué ocurre cuando se elimina dicho +1)\n",
        "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1]: \n",
        "            if neighbor != word:\n",
        "                data.append((word, neighbor))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVGhl12TpPEc"
      },
      "source": [
        "features = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "labels = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "for i,(feature,label) in enumerate(data):\n",
        "  features[i,vocabulary.index(feature)] = 1\n",
        "  labels[i,vocabulary.index(label)] = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1a0Y-szpRZ"
      },
      "source": [
        "class Word2Vec:\n",
        "  def __init__(self, vocab_size=0, embedding_dim=2, epochs=10000):\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.epochs=epochs\n",
        "    self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "  def train(self, x_train=None, y_train=None):\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
        " \n",
        "    self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
        " \n",
        "    for _ in range(self.epochs):\n",
        "      with tf.GradientTape() as t:\n",
        "        hidden_layer = tf.add(tf.matmul(x_train,self.W1),self.b1) \n",
        "        output_layer = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, self.W2), self.b2))\n",
        "        cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
        " \n",
        "      grads = t.gradient(cross_entropy_loss, [self.W1, self.b1, self.W2, self.b2])\n",
        "      self.optimizer.apply_gradients(zip(grads,[self.W1, self.b1, self.W2, self.b2]))\n",
        "      if(_ % 1000 == 0):\n",
        "        print(cross_entropy_loss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FswiyGsE0ETj",
        "outputId": "79dc9a2d-7f66-4f56-b505-0275770ce86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2v = Word2Vec(vocab_size=len(vocabulary), epochs=10000)\n",
        "w2v.train(features, labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.9014196, shape=(), dtype=float32)\n",
            "tf.Tensor(2.0284772, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8472476, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8111458, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7964778, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7883557, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7831641, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7795072, shape=(), dtype=float32)\n",
            "tf.Tensor(1.7767707, shape=(), dtype=float32)\n",
            "tf.Tensor(1.774647, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xazPhyRg6Xqi"
      },
      "source": [
        "# programa\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqZEv1-4RL7"
      },
      "source": [
        "## Embeddings con gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcmrlhNLkSC"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import logging"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pc9wPn4vQgL"
      },
      "source": [
        "Descargamos el corpus Brown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxoUU72HiAEb",
        "outputId": "72b8c8de-dd29-48f2-d637-7fca33336b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2y8Lb66OMHs"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoP29UJM60q"
      },
      "source": [
        "corpus = nltk.corpus.brown.sents()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqhjOSpvBUF"
      },
      "source": [
        "Entrenamos un modelo model sobre el corpus Brown, el cual contiene alrededor de 1.000.000 de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFDEXJlNDxV",
        "outputId": "b1c741cd-3a6f-4b34-9b74-fe0cf4c731ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = gensim.models.word2vec.Word2Vec(sentences = corpus)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 04:22:51,415 : INFO : collecting all words and their counts\n",
            "2020-11-08 04:22:51,420 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-11-08 04:22:51,959 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
            "2020-11-08 04:22:52,412 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
            "2020-11-08 04:22:52,889 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
            "2020-11-08 04:22:53,338 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
            "2020-11-08 04:22:53,694 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
            "2020-11-08 04:22:53,966 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
            "2020-11-08 04:22:53,967 : INFO : Loading a fresh vocabulary\n",
            "2020-11-08 04:22:54,013 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
            "2020-11-08 04:22:54,014 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
            "2020-11-08 04:22:54,069 : INFO : deleting the raw counts dictionary of 56057 items\n",
            "2020-11-08 04:22:54,076 : INFO : sample=0.001 downsamples 42 most-common words\n",
            "2020-11-08 04:22:54,078 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
            "2020-11-08 04:22:54,128 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
            "2020-11-08 04:22:54,130 : INFO : resetting layer weights\n",
            "2020-11-08 04:22:57,139 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-11-08 04:22:58,158 : INFO : EPOCH 1 - PROGRESS: at 23.64% examples, 190390 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:22:59,182 : INFO : EPOCH 1 - PROGRESS: at 47.04% examples, 196029 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:00,216 : INFO : EPOCH 1 - PROGRESS: at 71.94% examples, 198386 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:01,107 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 04:23:01,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 04:23:01,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 04:23:01,121 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 4.0s, 196487 effective words/s\n",
            "2020-11-08 04:23:02,139 : INFO : EPOCH 2 - PROGRESS: at 23.64% examples, 191846 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:03,158 : INFO : EPOCH 2 - PROGRESS: at 46.48% examples, 193827 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:04,195 : INFO : EPOCH 2 - PROGRESS: at 67.44% examples, 190296 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:05,206 : INFO : EPOCH 2 - PROGRESS: at 98.86% examples, 189845 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:05,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 04:23:05,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 04:23:05,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 04:23:05,255 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 4.1s, 189526 effective words/s\n",
            "2020-11-08 04:23:06,283 : INFO : EPOCH 3 - PROGRESS: at 23.64% examples, 189214 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:07,313 : INFO : EPOCH 3 - PROGRESS: at 47.04% examples, 194923 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:08,322 : INFO : EPOCH 3 - PROGRESS: at 70.81% examples, 197086 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:09,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 04:23:09,348 : INFO : EPOCH 3 - PROGRESS: at 99.80% examples, 190798 words/s, in_qsize 1, out_qsize 1\n",
            "2020-11-08 04:23:09,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 04:23:09,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 04:23:09,359 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 4.1s, 190699 effective words/s\n",
            "2020-11-08 04:23:10,385 : INFO : EPOCH 4 - PROGRESS: at 22.82% examples, 183296 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:11,411 : INFO : EPOCH 4 - PROGRESS: at 46.48% examples, 192347 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:12,433 : INFO : EPOCH 4 - PROGRESS: at 70.81% examples, 196699 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:13,365 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 04:23:13,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 04:23:13,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 04:23:13,383 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 4.0s, 194627 effective words/s\n",
            "2020-11-08 04:23:14,420 : INFO : EPOCH 5 - PROGRESS: at 23.64% examples, 187253 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:15,422 : INFO : EPOCH 5 - PROGRESS: at 46.48% examples, 193185 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:16,454 : INFO : EPOCH 5 - PROGRESS: at 70.81% examples, 196642 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 04:23:17,382 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 04:23:17,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 04:23:17,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 04:23:17,397 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 4.0s, 194972 effective words/s\n",
            "2020-11-08 04:23:17,398 : INFO : training on a 5805960 raw words (3908888 effective words) took 20.3s, 192960 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ntZpYYBvGFc"
      },
      "source": [
        "Importamos un modelo de word2vec entrenado sobre google news, este modelo fue entrenado sobre un corpus de 3.000.000.000 de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnP7Rh5jVpk",
        "outputId": "65f3c3a7-d7dd-4625-ea7d-485f61261733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "news = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 04:23:17,404 : INFO : Creating /root/gensim-data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 04:27:02,228 : INFO : word2vec-google-news-300 downloaded\n",
            "2020-11-08 04:27:02,230 : INFO : loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "2020-11-08 04:29:10,606 : INFO : loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REu7SPwCrKK2"
      },
      "source": [
        "## Actividad 2: Exploración de los embeddings\n",
        "\n",
        "Después de calcular los embeddings, podemos asociar un vector de números reales, de dimensiones conocidas a una palabra de nuestro vocabulario.\n",
        "\n",
        "\n",
        "1.   ¿Cuántas dimensiones tiene cada vector asociado a las palabras en cada uno de los modelos?\n",
        "2.   ¿Cuántos vectores hay en cada uno de los modelos?\n",
        "3.   ¿Existen diferencian en el tamaño del vocabulario del modelo `model` y el modelo news?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RtInP-wrHEn"
      },
      "source": [
        "# HINT\n",
        "# news.wv.vectors # esta es una matriz"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxFNhxZSoZ0"
      },
      "source": [
        "## Actividad 3: Similaridad y analogía\n",
        "\n",
        "Existen un par de cualidades semánticas de las palabras, las cuales pueden ser fácilmente demostradas a través de operaciones vectoriales sobre el espacio generado por el proceso de cálculo de embeddings.\n",
        "\n",
        "La similaridad es la métrica de cercanía que tienen 2 palabras, esta característica es fácil de representar a través de la similaridad coseno entre 2 vectores.\n",
        "\n",
        "La analogía es la relación semántica que tienen 2 palabras, por ejemplo, la palabra \"rey\" y \"reina\" están relacionadas por el concepto de \"género\". Estas analogías se pueden operacionalizar en el espacio vectorial de los word embeddings como la resta de los vectores asociados a las palabras.\n",
        "\n",
        "\n",
        "\n",
        "1.   Verifique cuáles son las palabras más cercanas a palabras que usted seleccione e interprete la correctitud de las palabras retornadas por los modelos `model` y `news`\n",
        "2.   Verifique cuál de los modelos resuelve mejor la prueba de analogía: \"man\" es a \"woman\" como \"king\" es a \"queen\". Invente otra analogía y pruebe si el modelo la puede resolver.\n",
        "3.   Según sus pruebas, ¿el tamaño del corpus de entrenamiento afecta el rendimiento del modelo?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fQ0w9XlNOHE",
        "outputId": "65983108-abff-4476-dde4-24027d425ba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# así obtenemos las palabras más cercanas a las palabra \"woman\"\n",
        "model.wv.most_similar(\"woman\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 04:29:10,633 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.9580943584442139),\n",
              " ('boy', 0.9006985425949097),\n",
              " ('man', 0.876850962638855),\n",
              " ('child', 0.8736554384231567),\n",
              " ('young', 0.8692346811294556),\n",
              " ('fellow', 0.8601439595222473),\n",
              " ('youngster', 0.858249306678772),\n",
              " ('remark', 0.8562021255493164),\n",
              " ('artist', 0.8556320071220398),\n",
              " ('old', 0.8539116382598877)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ov7uKOqrGR"
      },
      "source": [
        "#HINT:\n",
        "# news.wv.vectors[news.vocab[\"woman\"].index] # así obtenemos el vector asociado a la palabra woman\n",
        "# news.wv.similar_by_vector(vector) # así obtenemos las palabras más cercanas a un vector"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj4Hv0fkPKXg"
      },
      "source": [
        "## Actividad 4: Reducción de dimensionalidad y visualización\n",
        "\n",
        "Nuestros vectores tienen más de 2 dimensiones, por lo que no es muy fácil interptretar su ubicación espacial. Utiliza un método de reducción de dimensionalidad que conozcas para reducir hacia 2 dimensiones nuestros vectores y visualiza el resultado.\n",
        "\n",
        "\n",
        "*   ¿Existen agrupaciones aparentes de palabras dentro del espacio? y si es así, ¿las palabras agrupadas, son similares?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JhwXqJJeXaD"
      },
      "source": [
        "# prográmame\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}