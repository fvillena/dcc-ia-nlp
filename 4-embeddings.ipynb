{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPRviBCOoIUFFzO6O8h0YM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/dcc-ia-nlp/blob/master/4-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPcd3UfJ5ec"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sonMA0Csl_Z2"
      },
      "source": [
        "# Actividad 1: Word2vec en tensorflow\n",
        "\n",
        "Observe el preprocesamiento que se realiza al corpus, la estructura construida con Tensorflow y el procedimiento para entrenar el modelo.\n",
        "\n",
        "\n",
        "1.   Construya los word embeddings desde los pesos y sesgos del modelo entrenado\n",
        "2.   Visualice los embeddings bidimensionales en un gráfico de dispersión e interprete los resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zak8IEVRl8JD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrljKgFmJFj"
      },
      "source": [
        "corpus = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VnMZGnBmS4V"
      },
      "source": [
        "def preprocess(text):\n",
        "  tokens = text.split(\" \")\n",
        "  return [token for token in tokens if len(token) > 2]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6vpojsnI9L"
      },
      "source": [
        "def get_vocabulary(corpus):\n",
        "  vocabulary = []\n",
        "  for text in corpus:\n",
        "    vocabulary += text\n",
        "  return sorted(list(set(vocabulary)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InbdYdnmslf"
      },
      "source": [
        "sentences = list(map(preprocess, corpus))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaiaxx3Cm_eh"
      },
      "source": [
        "vocabulary = get_vocabulary(sentences)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCP-PFhln7wE"
      },
      "source": [
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        # Acá se toma una ventana de -WINDOWS_SIZE, WINDOWS_SIZE para generar el skip gram. Dado que las \n",
        "        # frases son cortas, se utiliza min y max para tener cuidado con los límites de la frase. \n",
        "        # Además, el +1 en el límite superior es para considerar el índice de la propia palabra en cuestión \n",
        "        # (probar qué ocurre cuando se elimina dicho +1)\n",
        "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1]: \n",
        "            if neighbor != word:\n",
        "                data.append((word, neighbor))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVGhl12TpPEc"
      },
      "source": [
        "features = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "labels = np.zeros((len(data),len(vocabulary)),dtype=np.float32)\n",
        "for i,(feature,label) in enumerate(data):\n",
        "  features[i,vocabulary.index(feature)] = 1\n",
        "  labels[i,vocabulary.index(label)] = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1a0Y-szpRZ"
      },
      "source": [
        "class Word2Vec:\n",
        "  def __init__(self, vocab_size=0, embedding_dim=2, epochs=10000):\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.epochs=epochs\n",
        "    self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
        "  def train(self, x_train=None, y_train=None):\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
        " \n",
        "    self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
        " \n",
        "    for _ in range(self.epochs):\n",
        "      with tf.GradientTape() as t:\n",
        "        hidden_layer = tf.add(tf.matmul(x_train,self.W1),self.b1) \n",
        "        output_layer = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, self.W2), self.b2))\n",
        "        cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
        " \n",
        "      grads = t.gradient(cross_entropy_loss, [self.W1, self.b1, self.W2, self.b2])\n",
        "      self.optimizer.apply_gradients(zip(grads,[self.W1, self.b1, self.W2, self.b2]))\n",
        "      if(_ % 1000 == 0):\n",
        "        print(cross_entropy_loss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FswiyGsE0ETj",
        "outputId": "8626ba87-49f2-47ba-d89b-6b095405ce34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2v = Word2Vec(vocab_size=len(vocabulary), epochs=10000)\n",
        "w2v.train(features, labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.8781688, shape=(), dtype=float32)\n",
            "tf.Tensor(2.0157373, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9512162, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9299581, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9181203, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9100775, shape=(), dtype=float32)\n",
            "tf.Tensor(1.9040195, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8990917, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8947947, shape=(), dtype=float32)\n",
            "tf.Tensor(1.8908191, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xazPhyRg6Xqi"
      },
      "source": [
        "# programa\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqZEv1-4RL7"
      },
      "source": [
        "## Embeddings con gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcmrlhNLkSC"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import logging"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pc9wPn4vQgL"
      },
      "source": [
        "Descargamos el corpus Brown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxoUU72HiAEb",
        "outputId": "bf591ca3-dddc-46d6-ca44-ded84e71910b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2y8Lb66OMHs"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoP29UJM60q"
      },
      "source": [
        "corpus = nltk.corpus.brown.sents()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqhjOSpvBUF"
      },
      "source": [
        "Entrenamos un modelo model sobre el corpus Brown, el cual contiene alrededor de 1.000.000 de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFDEXJlNDxV",
        "outputId": "50686fde-4fed-4faa-a0ab-f45012acb068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = gensim.models.word2vec.Word2Vec(sentences = corpus)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 03:25:43,766 : INFO : collecting all words and their counts\n",
            "2020-11-08 03:25:43,774 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-11-08 03:25:44,649 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
            "2020-11-08 03:25:45,447 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
            "2020-11-08 03:25:45,967 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
            "2020-11-08 03:25:46,395 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
            "2020-11-08 03:25:46,736 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
            "2020-11-08 03:25:47,009 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
            "2020-11-08 03:25:47,010 : INFO : Loading a fresh vocabulary\n",
            "2020-11-08 03:25:47,060 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
            "2020-11-08 03:25:47,061 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
            "2020-11-08 03:25:47,122 : INFO : deleting the raw counts dictionary of 56057 items\n",
            "2020-11-08 03:25:47,126 : INFO : sample=0.001 downsamples 42 most-common words\n",
            "2020-11-08 03:25:47,129 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
            "2020-11-08 03:25:47,175 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
            "2020-11-08 03:25:47,176 : INFO : resetting layer weights\n",
            "2020-11-08 03:25:50,170 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-11-08 03:25:51,175 : INFO : EPOCH 1 - PROGRESS: at 22.82% examples, 186261 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:52,183 : INFO : EPOCH 1 - PROGRESS: at 45.87% examples, 192340 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:53,184 : INFO : EPOCH 1 - PROGRESS: at 68.78% examples, 195898 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:54,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 03:25:54,195 : INFO : EPOCH 1 - PROGRESS: at 99.06% examples, 192735 words/s, in_qsize 1, out_qsize 1\n",
            "2020-11-08 03:25:54,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 03:25:54,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 03:25:54,203 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 4.0s, 193955 effective words/s\n",
            "2020-11-08 03:25:55,235 : INFO : EPOCH 2 - PROGRESS: at 23.64% examples, 188528 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:56,244 : INFO : EPOCH 2 - PROGRESS: at 45.87% examples, 189983 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:57,250 : INFO : EPOCH 2 - PROGRESS: at 67.44% examples, 191760 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:58,252 : INFO : EPOCH 2 - PROGRESS: at 94.58% examples, 184601 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:25:58,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 03:25:58,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 03:25:58,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 03:25:58,458 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 4.3s, 183999 effective words/s\n",
            "2020-11-08 03:25:59,493 : INFO : EPOCH 3 - PROGRESS: at 23.64% examples, 187358 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:00,522 : INFO : EPOCH 3 - PROGRESS: at 47.04% examples, 194124 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:01,553 : INFO : EPOCH 3 - PROGRESS: at 71.94% examples, 197238 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:02,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 03:26:02,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 03:26:02,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 03:26:02,486 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 4.0s, 194201 effective words/s\n",
            "2020-11-08 03:26:03,520 : INFO : EPOCH 4 - PROGRESS: at 24.43% examples, 194451 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:04,535 : INFO : EPOCH 4 - PROGRESS: at 47.85% examples, 198901 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:05,543 : INFO : EPOCH 4 - PROGRESS: at 71.94% examples, 199782 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:06,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 03:26:06,434 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 03:26:06,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 03:26:06,442 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 4.0s, 197841 effective words/s\n",
            "2020-11-08 03:26:07,464 : INFO : EPOCH 5 - PROGRESS: at 23.64% examples, 190372 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:08,479 : INFO : EPOCH 5 - PROGRESS: at 47.04% examples, 196854 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:09,506 : INFO : EPOCH 5 - PROGRESS: at 71.94% examples, 199422 words/s, in_qsize 0, out_qsize 0\n",
            "2020-11-08 03:26:10,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-11-08 03:26:10,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-11-08 03:26:10,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-11-08 03:26:10,414 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 4.0s, 197119 effective words/s\n",
            "2020-11-08 03:26:10,415 : INFO : training on a 5805960 raw words (3908888 effective words) took 20.2s, 193089 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ntZpYYBvGFc"
      },
      "source": [
        "Importamos un modelo de word2vec entrenado sobre google news, este modelo fue entrenado sobre un corpus de 3.000.000.000 de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnP7Rh5jVpk",
        "outputId": "d3d31b4b-77d7-47ab-d240-91ef05edcffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "news = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 03:26:10,422 : INFO : Creating /root/gensim-data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[===========================-----------------------] 55.4% 920.6/1662.8MB downloadedBuffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REu7SPwCrKK2"
      },
      "source": [
        "## Actividad 2: Exploración de los embeddings\n",
        "\n",
        "Después de calcular los embeddings, podemos asociar un vector de números reales, de dimensiones conocidas a una palabra de nuestro vocabulario.\n",
        "\n",
        "\n",
        "1.   ¿Cuántas dimensiones tiene cada vector asociado a las palabras en cada uno de los modelos?\n",
        "2.   ¿Cuántos vectores hay en cada uno de los modelos?\n",
        "3.   ¿Existen diferencian en el tamaño del vocabulario del modelo `model` y el modelo news?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RtInP-wrHEn"
      },
      "source": [
        "# HINT\n",
        "# news.wv.vectors # esta es una matriz"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxFNhxZSoZ0"
      },
      "source": [
        "## Actividad 3: Similaridad y analogía\n",
        "\n",
        "Existen un par de cualidades semánticas de las palabras, las cuales pueden ser fácilmente demostradas a través de operaciones vectoriales sobre el espacio generado por el proceso de cálculo de embeddings.\n",
        "\n",
        "La similaridad es la métrica de cercanía que tienen 2 palabras, esta característica es fácil de representar a través de la similaridad coseno entre 2 vectores.\n",
        "\n",
        "La analogía es la relación semántica que tienen 2 palabras, por ejemplo, la palabra \"rey\" y \"reina\" están relacionadas por el concepto de \"género\". Estas analogías se pueden operacionalizar en el espacio vectorial de los word embeddings como la resta de los vectores asociados a las palabras.\n",
        "\n",
        "\n",
        "\n",
        "1.   Verifique cuáles son las palabras más cercanas a palabras que usted seleccione e interprete la correctitud de las palabras retornadas por los modelos `model` y `news`\n",
        "2.   Verifique cuál de los modelos resuelve mejor la prueba de analogía: \"man\" es a \"woman\" como \"king\" es a \"queen\". Invente otra analogía y pruebe si el modelo la puede resolver.\n",
        "3.   Según sus pruebas, ¿el tamaño del corpus de entrenamiento afecta el rendimiento del modelo?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fQ0w9XlNOHE",
        "outputId": "0d0dc37e-086a-4cfd-99cf-bf05d285a7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# así obtenemos las palabras más cercanas a las palabra \"woman\"\n",
        "model.wv.most_similar(\"woman\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 03:32:17,659 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('girl', 0.9568180441856384),\n",
              " ('boy', 0.9010509252548218),\n",
              " ('child', 0.877415657043457),\n",
              " ('man', 0.8762867450714111),\n",
              " ('young', 0.873813807964325),\n",
              " ('distaste', 0.8667408227920532),\n",
              " ('artist', 0.8614952564239502),\n",
              " ('fellow', 0.8577835559844971),\n",
              " ('youngster', 0.856518030166626),\n",
              " ('conversation', 0.8556602597236633)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ov7uKOqrGR"
      },
      "source": [
        "#HINT:\n",
        "# news.wv.vectors[news.vocab[\"woman\"].index] # así obtenemos el vector asociado a la palabra woman\n",
        "# news.wv.similar_by_vector(vector) # así obtenemos las palabras más cercanas a un vector"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj4Hv0fkPKXg"
      },
      "source": [
        "## Actividad 4: Reducción de dimensionalidad y visualización\n",
        "\n",
        "Nuestros vectores tienen más de 2 dimensiones, por lo que no es muy fácil interptretar su ubicación espacial. Utiliza un método de reducción de dimensionalidad que conozcas para reducir hacia 2 dimensiones nuestros vectores y visualiza el resultado.\n",
        "\n",
        "\n",
        "*   ¿Existen agrupaciones aparentes de palabras dentro del espacio? y si es así, ¿las palabras agrupadas, son similares?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JhwXqJJeXaD"
      },
      "source": [
        "# prográmame\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}